[
    {
        "id": 1,
        "domain": "Design Secure Architectures",
        "questionText": "A company wants to host a static website on AWS. They need a solution that is highly available, scalable, and secure. Which of the following AWS services would be most appropriate for this use case?",
        "options": [
            {
                "letter": "A",
                "text": "Amazon EC2 instances with a load balancer"
            },
            {
                "letter": "B",
                "text": "Amazon S3 with CloudFront for content delivery"
            },
            {
                "letter": "C",
                "text": "AWS Lambda with API Gateway"
            },
            {
                "letter": "D",
                "text": "Amazon RDS for storing website content"
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **Amazon S3 (Simple Storage Service):** S3 is an object storage service that offers industry-leading scalability, data availability, security, and performance. It is ideal for hosting static websites because it can serve HTML, CSS, JavaScript, and other static files directly.\n*   **Amazon CloudFront:** CloudFront is a content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency and high transfer speeds. When combined with S3, CloudFront caches the website content at edge locations closer to users, improving performance and reducing latency.\n\n**Why other options are incorrect:**\n\n*   **A) Amazon EC2 instances with a load balancer:** While EC2 instances can host websites, it's an overkill for a static website and involves more management overhead (patching, scaling, etc.) compared to S3. Load balancers are typically used for dynamic content and distributing traffic across multiple instances.\n*   **C) AWS Lambda with API Gateway:** This combination is suitable for serverless applications and APIs, but not for hosting static website content directly. While Lambda can be used to serve dynamic content, it's not the primary choice for static websites.\n*   **D) Amazon RDS for storing website content:** RDS is a relational database service. It is not designed for hosting website content like HTML files or images. S3 is the appropriate service for storing these types of files."
    },
    {
        "id": 2,
        "domain": "Design Resilient Architectures",
        "questionText": "A company is deploying a critical application on AWS that requires high availability and fault tolerance. The application consists of a web tier, an application tier, and a database tier. Which of the following design choices would best ensure high availability for this application?",
        "options": [
            {
                "letter": "A",
                "text": "Deploying all tiers in a single Availability Zone."
            },
            {
                "letter": "B",
                "text": "Using a single, large EC2 instance for all tiers."
            },
            {
                "letter": "C",
                "text": "Deploying the application across multiple Availability Zones with load balancing and auto-scaling."
            },
            {
                "letter": "D",
                "text": "Storing all application data on a single EBS volume."
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **Multiple Availability Zones:** Distributing application components across multiple Availability Zones (AZs) ensures that if one AZ fails, the application can continue to operate from other AZs. This is a core principle of designing resilient architectures on AWS.\n*   **Load Balancing:** Load balancers distribute incoming traffic across multiple instances, preventing any single instance from becoming a bottleneck and improving fault tolerance.\n*   **Auto Scaling:** Auto Scaling automatically adjusts the number of EC2 instances in response to changing demand, ensuring that the application has enough capacity to handle traffic and maintain performance.\n\n**Why other options are incorrect:**\n\n*   **A) Deploying all tiers in a single Availability Zone:** This creates a single point of failure. If the AZ experiences an outage, the entire application will be unavailable.\n*   **B) Using a single, large EC2 instance for all tiers:** This also creates a single point of failure. If the instance fails, the entire application will be down. Additionally, it limits scalability and may not be cost-effective.\n*   **D) Storing all application data on a single EBS volume:** EBS volumes are zonal resources. If the AZ where the volume is located fails, the data will be inaccessible. For high availability, data should be replicated across multiple AZs or stored in a multi-AZ database service like Amazon RDS Multi-AZ."
    },
    {
        "id": 3,
        "domain": "Design High-Performing Architectures",
        "questionText": "A company is launching a new mobile game that is expected to have a large number of concurrent users. The game requires low-latency access to user data and leaderboards. Which AWS database service would be most suitable for this scenario?",
        "options": [
            {
                "letter": "A",
                "text": "Amazon RDS (Relational Database Service)"
            },
            {
                "letter": "B",
                "text": "Amazon Redshift"
            },
            {
                "letter": "C",
                "text": "Amazon DynamoDB"
            },
            {
                "letter": "D",
                "text": "Amazon S3"
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **Amazon DynamoDB:** DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. It is designed for applications that require consistent, single-digit millisecond latency at any scale, making it ideal for gaming applications, mobile backends, and real-time bidding.\n\n**Why other options are incorrect:**\n\n*   **A) Amazon RDS:** While RDS offers various relational database engines, it might not be the best choice for high-volume, low-latency access required by a popular mobile game. DynamoDB is specifically designed for these types of workloads.\n*   **B) Amazon Redshift:** Redshift is a data warehousing service, optimized for analytical queries rather than transactional workloads or real-time data access.\n*   **D) Amazon S3:** S3 is an object storage service, not a database. While it can store data, it doesn't provide the querying capabilities or low-latency access needed for a dynamic mobile game backend."
    },
    {
        "id": 4,
        "domain": "Design Cost-Optimized Architectures",
        "questionText": "A company wants to reduce the cost of its AWS infrastructure. They have several EC2 instances that are used for development and testing. These instances are only needed during business hours (Monday to Friday, 9 AM to 5 PM). Which of the following strategies would be most effective in reducing costs?",
        "options": [
            {
                "letter": "A",
                "text": "Using On-Demand instances and stopping them when not in use."
            },
            {
                "letter": "B",
                "text": "Using Reserved Instances for all development and testing instances."
            },
            {
                "letter": "C",
                "text": "Using Spot Instances for all development and testing instances."
            },
            {
                "letter": "D",
                "text": "Using Dedicated Hosts for all development and testing instances."
            }
        ],
        "correctAnswerLetter": "A",
        "explanation": "*   **A) Using On-Demand instances and stopping them when not in use:** This is the most cost-effective approach for workloads that are not needed 24/7. By stopping instances when they are not in use, you only pay for the time the instances are running. You can automate this process using scripts or AWS services like AWS Lambda.\n*   **B) Using Reserved Instances for all development and testing instances:** Reserved Instances offer significant discounts compared to On-Demand pricing, but they require a commitment of 1 or 3 years. This might not be ideal for development and testing environments where needs can change frequently.\n*   **C) Using Spot Instances for all development and testing instances:** Spot Instances can provide significant cost savings, but they can be interrupted with little notice. This makes them unsuitable for critical development or testing tasks that require uninterrupted access.\n*   **D) Using Dedicated Hosts for all development and testing instances:** Dedicated Hosts are physical servers dedicated to your use. They are typically more expensive than other options and are used for specific compliance or licensing requirements."
    },
    {
        "id": 5,
        "domain": "Design Secure Architectures - Advanced",
        "questionText": "A company is building a multi-tier web application on AWS. The application consists of a public-facing web tier, an application tier, and a database tier. Which of the following security configurations is most appropriate for this architecture?",
        "options": [
            {
                "letter": "A",
                "text": "Place all tiers in a single public subnet, and use security groups to control traffic between tiers."
            },
            {
                "letter": "B",
                "text": "Place the web tier in a public subnet, and the application and database tiers in private subnets. Use security groups and NACLs to control traffic flow."
            },
            {
                "letter": "C",
                "text": "Place all tiers in private subnets, and use a VPN connection to allow users to access the application."
            },
            {
                "letter": "D",
                "text": "Use a single security group for all instances, and allow all traffic from any source."
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **B) Place the web tier in a public subnet, and the application and database tiers in private subnets. Use security groups and NACLs to control traffic flow.** This is a standard and secure architecture. The public subnet allows the web tier to be accessible from the internet, while the private subnets protect the application and database tiers from direct internet access. Security groups and NACLs provide granular control over traffic flow between tiers.\n*   **A) Place all tiers in a single public subnet, and use security groups to control traffic flow.** This is less secure because it exposes all tiers to the public internet, increasing the attack surface.\n*   **C) Place all tiers in private subnets, and use a VPN connection to allow users to access the application.** While this is a secure approach, it\n\n is not suitable for a public-facing web application. VPNs are typically used for secure access to private resources, not for public web traffic.\n*   **D) Use a single security group for all instances, and allow all traffic from any source.** This is a highly insecure configuration that exposes all instances to unnecessary risks."
    },
    {
        "id": 6,
        "domain": "Design Resilient Architectures - Advanced",
        "questionText": "A company runs a critical database on an Amazon RDS for MySQL instance. They need to ensure that the database can withstand an Availability Zone outage with minimal downtime and no data loss. Which RDS feature should they enable?",
        "options": [
            {
                "letter": "A",
                "text": "Read Replicas"
            },
            {
                "letter": "B",
                "text": "Multi-AZ deployment"
            },
            {
                "letter": "C",
                "text": "Automated Backups"
            },
            {
                "letter": "D",
                "text": "Enhanced Monitoring"
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **B) Multi-AZ deployment:** When you provision a Multi-AZ DB instance, Amazon RDS automatically creates a primary DB instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). In case of an infrastructure failure, Amazon RDS performs an automatic failover to the standby instance, so that you can resume database operations as soon as the failover is complete. This provides high availability and durability for your database.\n*   **A) Read Replicas:** Read Replicas are used to offload read traffic from the primary database instance and improve read scalability. They do not provide automatic failover in case of an AZ outage.\n*   **C) Automated Backups:** Automated backups allow you to recover your database to any point in time within the retention period. However, they do not provide immediate failover in case of an AZ outage. Restoring from a backup can take time.\n*   **D) Enhanced Monitoring:** Enhanced Monitoring provides real-time metrics about your DB instance, but it does not directly contribute to high availability or fault tolerance."
    },
    {
        "id": 7,
        "domain": "Design High-Performing Architectures - Advanced",
        "questionText": "A company has a web application that experiences sudden, unpredictable spikes in traffic. They want to ensure that the application can handle these spikes without performance degradation. Which AWS service or feature would be most effective in achieving this?",
        "options": [
            {
                "letter": "A",
                "text": "Amazon ElastiCache"
            },
            {
                "letter": "B",
                "text": "AWS Auto Scaling"
            },
            {
                "letter": "C",
                "text": "Amazon S3 Transfer Acceleration"
            },
            {
                "letter": "D",
                "text": "AWS Direct Connect"
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **B) AWS Auto Scaling:** AWS Auto Scaling monitors your applications and automatically adjusts capacity to maintain steady, predictable performance at the lowest possible cost. It can scale out (add instances) during traffic spikes and scale in (remove instances) when traffic subsides, ensuring that the application has the right amount of resources to handle the load.\n*   **A) Amazon ElastiCache:** ElastiCache is a caching service that can improve application performance by storing frequently accessed data in memory. While it can help with performance, it doesn't directly address the need to scale the application infrastructure in response to traffic spikes.\n*   **C) Amazon S3 Transfer Acceleration:** S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. It is not relevant to scaling an application to handle traffic spikes.\n*   **D) AWS Direct Connect:** AWS Direct Connect is a dedicated network connection from your premises to AWS. It can improve network performance and reduce costs, but it doesn't directly address application scalability."
    },
    {
        "id": 8,
        "domain": "Design Cost-Optimized Architectures - Advanced",
        "questionText": "A company is using Amazon S3 to store large amounts of data that is accessed infrequently. They want to reduce storage costs while ensuring that the data can be retrieved within a few hours if needed. Which S3 storage class would be most appropriate for this scenario?",
        "options": [
            {
                "letter": "A",
                "text": "S3 Standard"
            },
            {
                "letter": "B",
                "text": "S3 Intelligent-Tiering"
            },
            {
                "letter": "C",
                "text": "S3 Glacier Flexible Retrieval (formerly S3 Glacier)"
            },
            {
                "letter": "D",
                "text": "S3 One Zone-Infrequent Access"
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **C) S3 Glacier Flexible Retrieval:** This storage class is designed for archiving data that is rarely accessed but requires retrieval within minutes to hours. It offers low storage costs and flexible retrieval options. This aligns with the company's requirements for infrequent access and retrieval within a few hours.\n*   **A) S3 Standard:** S3 Standard is designed for frequently accessed data and offers high durability, availability, and performance. It is more expensive than S3 Glacier Flexible Retrieval for storing infrequently accessed data.\n*   **B) S3 Intelligent-Tiering:** S3 Intelligent-Tiering automatically moves data to the most cost-effective access tier based on access patterns. While it can optimize costs, S3 Glacier Flexible Retrieval is a more direct fit if the access pattern is known to be infrequent and retrieval times of a few hours are acceptable.\n*   **D) S3 One Zone-Infrequent Access:** S3 One Zone-IA is for data that is accessed less frequently but requires rapid access when needed. It stores data in a single AZ and is not resilient to AZ failure, making it less suitable for data that needs to be durable and retrievable even in the event of an AZ outage."
    },
    {
        "id": 9,
        "domain": "Design Secure Architectures - Scenario",
        "questionText": "A financial services company is migrating its customer relationship management (CRM) application to AWS. The CRM application handles sensitive customer data and must comply with strict regulatory requirements. The company needs to ensure that all data stored in Amazon S3 is encrypted at rest. Which of the following S3 encryption options provides the most control over encryption keys?",
        "options": [
            {
                "letter": "A",
                "text": "Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)"
            },
            {
                "letter": "B",
                "text": "Server-Side Encryption with AWS Key Management Service (SSE-KMS)"
            },
            {
                "letter": "C",
                "text": "Server-Side Encryption with Customer-Provided Keys (SSE-C)"
            },
            {
                "letter": "D",
                "text": "Client-Side Encryption"
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **C) Server-Side Encryption with Customer-Provided Keys (SSE-C):** With SSE-C, you manage the encryption keys yourself. Amazon S3 performs the encryption and decryption of your objects using the keys you provide. This option gives you the most control over your encryption keys, which can be important for meeting strict regulatory requirements.\n*   **A) Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3):** With SSE-S3, Amazon S3 manages the encryption keys. This is the simplest option but provides less control than SSE-KMS or SSE-C.\n*   **B) Server-Side Encryption with AWS Key Management Service (SSE-KMS):** With SSE-KMS, you use AWS KMS to manage your encryption keys. This provides more control than SSE-S3, including the ability to create and manage customer master keys (CMKs) and define usage policies. However, SSE-C offers even more direct control as the customer manages the keys entirely outside of AWS.\n*   **D) Client-Side Encryption:** Client-side encryption involves encrypting data before sending it to S3. While this provides strong control, the question specifically asks about S3 server-side encryption options. SSE-C is the server-side option that gives the most control over keys."
    },
    {
        "id": 10,
        "domain": "Design Resilient Architectures - Scenario",
        "questionText": "A global e-commerce company wants to deploy its application in multiple AWS Regions to provide low latency for users worldwide and ensure business continuity in case of a regional outage. Which AWS services and features would be essential for this multi-Region architecture?",
        "options": [
            {
                "letter": "A",
                "text": "Amazon Route 53 with latency-based routing, and S3 Cross-Region Replication."
            },
            {
                "letter": "B",
                "text": "Elastic Load Balancing within a single Region, and EC2 Auto Scaling."
            },
            {
                "letter": "C",
                "text": "AWS Direct Connect to connect all Regions, and Amazon RDS in a single Region."
            },
            {
                "letter": "D",
                "text": "AWS Storage Gateway to replicate data between Regions, and Amazon CloudFront with a single origin."
            }
        ],
        "correctAnswerLetter": "A",
        "explanation": "*   **A) Amazon Route 53 with latency-based routing, and S3 Cross-Region Replication:**\n    *   **Amazon Route 53 with latency-based routing:** This DNS service can route users to the AWS Region that provides the lowest latency, improving user experience.\n    *   **S3 Cross-Region Replication (CRR):** CRR automatically replicates data across S3 buckets in different Regions, ensuring data durability and availability in case of a regional outage.\n    *   Other components for a full multi-region active-active or active-passive setup would include replicating databases (e.g., DynamoDB Global Tables, RDS Cross-Region Read Replicas with promotion capabilities) and application stacks in each region.\n*   **B) Elastic Load Balancing within a single Region, and EC2 Auto Scaling:** These are important for regional resilience but do not address multi-Region requirements for global low latency or regional disaster recovery.\n*   **C) AWS Direct Connect to connect all Regions, and Amazon RDS in a single Region:** Direct Connect is for dedicated network connections to AWS, not typically for inter-Region connectivity for public applications. Having RDS in a single Region creates a single point of failure for the database in a multi-Region strategy.\n*   **D) AWS Storage Gateway to replicate data between Regions, and Amazon CloudFront with a single origin:** Storage Gateway is primarily for hybrid cloud storage. CloudFront with a single origin doesn't leverage a multi-Region application deployment for resilience or latency benefits for dynamic content."
    },
    {
        "id": 11,
        "domain": "Design High-Performing Architectures - Scenario",
        "questionText": "A data analytics company processes large datasets using a fleet of Amazon EC2 instances. The processing jobs are batch-oriented and can tolerate interruptions. The company wants to minimize the cost of these processing jobs. Which EC2 purchasing option would be most suitable?",
        "options": [
            {
                "letter": "A",
                "text": "On-Demand Instances"
            },
            {
                "letter": "B",
                "text": "Reserved Instances"
            },
            {
                "letter": "C",
                "text": "Spot Instances"
            },
            {
                "letter": "D",
                "text": "Dedicated Hosts"
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **C) Spot Instances:** Spot Instances allow you to bid on unused EC2 capacity and can provide significant cost savings (up to 90% off On-Demand prices). Since the processing jobs are batch-oriented and can tolerate interruptions, Spot Instances are an excellent choice for minimizing costs. If a Spot Instance is interrupted, the job can be restarted or resumed on another Spot Instance or an On-Demand instance.\n*   **A) On-Demand Instances:** On-Demand Instances are charged by the hour or second with no long-term commitments. They are more expensive than Spot Instances for fault-tolerant workloads.\n*   **B) Reserved Instances:** Reserved Instances provide a discount in exchange for a 1-year or 3-year commitment. They are suitable for steady-state workloads but may not be the most cost-effective for batch jobs that can leverage the deeper discounts of Spot Instances.\n*   **D) Dedicated Hosts:** Dedicated Hosts are physical servers dedicated to your use. They are the most expensive option and are typically used for specific compliance or licensing requirements, not for general cost optimization of batch processing."
    },
    {
        "id": 12,
        "domain": "Design Cost-Optimized Architectures - Scenario",
        "questionText": "A startup is developing a new web application with unpredictable traffic patterns. They want to use a compute service that automatically scales to handle traffic fluctuations and only charges for the actual compute time used, without needing to manage servers. Which AWS compute service best fits these requirements?",
        "options": [
            {
                "letter": "A",
                "text": "Amazon EC2"
            },
            {
                "letter": "B",
                "text": "AWS Lambda"
            },
            {
                "letter": "C",
                "text": "Amazon Lightsail"
            },
            {
                "letter": "D",
                "text": "AWS Elastic Beanstalk"
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **B) AWS Lambda:** AWS Lambda is a serverless compute service that lets you run code without provisioning or managing servers. It automatically scales your application by running code in response to triggers, and you pay only for the compute time consumed. This makes it ideal for applications with unpredictable traffic patterns and a desire to minimize costs and operational overhead.\n*   **A) Amazon EC2:** EC2 provides virtual servers, but you are responsible for managing the servers, scaling, and you pay for the instances while they are running, regardless of whether they are processing requests.\n*   **C) Amazon Lightsail:** Lightsail provides a simplified way to launch and manage virtual private servers, but it still involves managing servers and doesn't offer the same pay-per-use granularity and automatic scaling as Lambda for event-driven workloads.\n*   **D) AWS Elastic Beanstalk:** Elastic Beanstalk is a PaaS offering that simplifies deploying and scaling web applications. While it handles scaling, it still provisions underlying EC2 instances, and you pay for those instances. Lambda offers a more purely serverless and pay-per-request model."
    },
    {
        "id": 13,
        "domain": "Design Secure Architectures - Scenario",
        "questionText": "A company is deploying a new application that processes sensitive financial data. They need to ensure that network traffic between their Amazon VPC and their on-premises data center is encrypted and private. Which AWS service or feature should they use to establish this secure connection?",
        "options": [
            {
                "letter": "A",
                "text": "Amazon S3 VPC Endpoint"
            },
            {
                "letter": "B",
                "text": "AWS Direct Connect with public VIF"
            },
            {
                "letter": "C",
                "text": "AWS Site-to-Site VPN"
            },
            {
                "letter": "D",
                "text": "Network Load Balancer with TLS termination"
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **AWS Site-to-Site VPN:** This service creates a secure, encrypted tunnel between your VPC and your on-premises network over the internet. It uses IPsec to encrypt traffic in transit, ensuring privacy and data integrity. This is the standard solution for secure connectivity between a VPC and an on-premises environment when a dedicated private connection like Direct Connect is not required or is being supplemented.\n\n**Why other options are incorrect:**\n\n*   **A) Amazon S3 VPC Endpoint:** This allows instances in your VPC to access S3 privately without going over the internet, but it doesn't establish a secure connection to an on-premises data center.\n*   **B) AWS Direct Connect with public VIF:** AWS Direct Connect provides a dedicated network connection. A public Virtual Interface (VIF) allows access to public AWS services (like S3, EC2 APIs), but for private, encrypted traffic to a VPC, a private VIF or a transit VIF would be used, often in conjunction with VPN for encryption if the Direct Connect link itself isn't encrypted end-to-end by other means.\n*   **D) Network Load Balancer with TLS termination:** NLBs distribute traffic to targets within AWS. While TLS termination secures traffic between clients and the NLB, it doesn't address the secure connection requirement between the VPC and the on-premises data center."
    },
    {
        "id": 14,
        "domain": "Design Resilient Architectures - Scenario",
        "questionText": "A company's application uses an Amazon Aurora database. To improve disaster recovery capabilities, they want to replicate the database to another AWS Region with a low Recovery Point Objective (RPO) and Recovery Time Objective (RTO). Which Aurora feature best meets this requirement?",
        "options": [
            {
                "letter": "A",
                "text": "Aurora Read Replicas within the same Region."
            },
            {
                "letter": "B",
                "text": "Aurora Global Database."
            },
            {
                "letter": "C",
                "text": "Automated snapshots copied to another Region."
            },
            {
                "letter": "D",
                "text": "Manually creating a new cluster in another Region and restoring from a snapshot."
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **Aurora Global Database:** This feature is designed for globally distributed applications, allowing a single Aurora database to span multiple AWS Regions. It provides low-latency global reads and disaster recovery from Region-wide outages. Data is replicated with typical latency of under a second, providing a low RPO. Failover to a secondary Region can typically be completed in less than a minute, providing a low RTO.\n\n**Why other options are incorrect:**\n\n*   **A) Aurora Read Replicas within the same Region:** These improve read scalability and availability within a single Region but do not provide cross-Region disaster recovery.\n*   **C) Automated snapshots copied to another Region:** While copying snapshots provides a DR mechanism, the RPO is determined by the snapshot frequency and copy time, and RTO involves restoring the snapshot, which can be longer than an Aurora Global Database failover.\n*   **D) Manually creating a new cluster in another Region and restoring from a snapshot:** This is a manual DR process with a higher RPO and RTO compared to Aurora Global Database."
    },
    {
        "id": 15,
        "domain": "Design High-Performing Architectures - Scenario",
        "questionText": "A company is running a high-performance computing (HPC) workload on a cluster of EC2 instances. The workload requires very low-latency network communication between instances. Which EC2 networking feature should they leverage?",
        "options": [
            {
                "letter": "A",
                "text": "Elastic IP Addresses"
            },
            {
                "letter": "B",
                "text": "Placement Groups (Cluster)"
            },
            {
                "letter": "C",
                "text": "VPC Peering"
            },
            {
                "letter": "D",
                "text": "NAT Gateway"
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **Placement Groups (Cluster):** A cluster placement group is a logical grouping of instances within a single Availability Zone. Cluster placement groups are recommended for applications that need low network latency, high network throughput, or both between instances. Instances in a cluster placement group are placed in close proximity to each other to achieve the best network performance.\n\n**Why other options are incorrect:**\n\n*   **A) Elastic IP Addresses:** These are static public IPv4 addresses for dynamic cloud computing. They don't inherently improve inter-instance network latency.\n*   **C) VPC Peering:** This allows network connectivity between two VPCs. While it enables communication, it doesn't optimize for the ultra-low latency required by HPC workloads within a single cluster in the same way a cluster placement group does.\n*   **D) NAT Gateway:** This enables instances in a private subnet to connect to the internet or other AWS services, but prevents the internet from initiating a connection with those instances. It's not relevant for optimizing inter-instance communication for HPC."
    },
    {
        "id": 16,
        "domain": "Design Cost-Optimized Architectures - Scenario",
        "questionText": "A company has a large amount of historical data stored in Amazon S3 Standard. This data is accessed very rarely (once a year or less) but must be retained for compliance reasons for 7 years. Retrieval times of 12-48 hours are acceptable. Which S3 storage class offers the lowest storage cost for this scenario?",
        "options": [
            {
                "letter": "A",
                "text": "S3 Standard-Infrequent Access (S3 Standard-IA)"
            },
            {
                "letter": "B",
                "text": "S3 One Zone-Infrequent Access (S3 One Zone-IA)"
            },
            {
                "letter": "C",
                "text": "S3 Glacier Flexible Retrieval"
            },
            {
                "letter": "D",
                "text": "S3 Glacier Deep Archive"
            }
        ],
        "correctAnswerLetter": "D",
        "explanation": "*   **S3 Glacier Deep Archive:** This is Amazon S3\u2019s lowest-cost storage class and supports long-term retention and digital preservation for data that may be accessed once or twice in a year. It is designed for customers that retain data sets for 7-10 years or longer and can tolerate retrieval times of 12-48 hours.\n\n**Why other options are incorrect:**\n\n*   **A) S3 Standard-Infrequent Access (S3 Standard-IA):** Suitable for data that is accessed less frequently but requires rapid access when needed. It has higher storage costs than S3 Glacier Deep Archive.\n*   **B) S3 One Zone-Infrequent Access (S3 One Zone-IA):** Similar to S3 Standard-IA but stores data in a single AZ, making it less resilient and not ideal for long-term compliance data that must be retained.\n*   **C) S3 Glacier Flexible Retrieval:** Offers low-cost storage for archives with retrieval options from minutes to hours. While cheaper than S3 Standard-IA, S3 Glacier Deep Archive is even lower cost for data that can tolerate longer retrieval times."
    },
    {
        "id": 17,
        "domain": "Design Secure Architectures - Scenario",
        "questionText": "A company wants to provide its developers with temporary, limited-privilege credentials to access specific AWS resources without creating long-term IAM users. Which AWS service should be used to achieve this?",
        "options": [
            {
                "letter": "A",
                "text": "AWS Identity and Access Management (IAM) Roles"
            },
            {
                "letter": "B",
                "text": "AWS Secrets Manager"
            },
            {
                "letter": "C",
                "text": "AWS Single Sign-On (AWS SSO)"
            },
            {
                "letter": "D",
                "text": "Amazon Cognito User Pools"
            }
        ],
        "correctAnswerLetter": "A",
        "explanation": "*   **AWS Identity and Access Management (IAM) Roles:** IAM roles are a secure way to grant permissions to entities that you trust. Roles can be assumed by IAM users, applications, or AWS services to obtain temporary security credentials. This is ideal for granting developers temporary access with specific permissions without managing long-term credentials for them directly for every task.\n\n**Why other options are incorrect:**\n\n*   **B) AWS Secrets Manager:** This service helps you protect secrets needed to access your applications, services, and IT resources. While it manages credentials, it doesn't directly provide temporary access credentials for users in the way IAM roles do.\n*   **C) AWS Single Sign-On (AWS SSO):** AWS SSO makes it easy to centrally manage access to multiple AWS accounts and business applications. While it uses IAM roles behind the scenes for AWS account access, the core mechanism for temporary, limited-privilege credentials is the IAM role itself.\n*   **D) Amazon Cognito User Pools:** These are user directories that provide sign-up and sign-in options for your web and mobile app users. They are not primarily for granting AWS resource access to developers."
    },
    {
        "id": 18,
        "domain": "Design Resilient Architectures - Scenario",
        "questionText": "A company is running a web application on Amazon EC2 instances behind an Application Load Balancer (ALB). They want to ensure that if an entire Availability Zone (AZ) becomes unavailable, the application remains accessible. What is the minimum requirement to achieve this?",
        "options": [
            {
                "letter": "A",
                "text": "Configure the ALB to distribute traffic to instances in at least two AZs."
            },
            {
                "letter": "B",
                "text": "Enable cross-zone load balancing on the ALB."
            },
            {
                "letter": "C",
                "text": "Deploy EC2 instances in a single AZ but use a large instance type."
            },
            {
                "letter": "D",
                "text": "Use an Elastic IP address for the ALB."
            }
        ],
        "correctAnswerLetter": "A",
        "explanation": "*   **Configure the ALB to distribute traffic to instances in at least two AZs:** To ensure high availability and fault tolerance against an AZ failure, the application instances must be deployed across multiple AZs, and the load balancer must be configured to distribute traffic to instances in those AZs. If one AZ fails, the ALB will route traffic to the healthy instances in the other AZ(s).\n\n**Why other options are incorrect:**\n\n*   **B) Enable cross-zone load balancing on the ALB:** Cross-zone load balancing distributes traffic evenly across all registered instances in all enabled AZs. While beneficial for even distribution, the primary requirement for AZ resilience is having instances in multiple AZs to begin with.\n*   **C) Deploy EC2 instances in a single AZ but use a large instance type:** This creates a single point of failure at the AZ level. If that AZ fails, the application becomes unavailable regardless of instance size.\n*   **D) Use an Elastic IP address for the ALB:** ALBs have DNS names, not Elastic IP addresses. While Network Load Balancers can have static IPs, this doesn't address AZ resilience for the application instances."
    },
    {
        "id": 19,
        "domain": "Design High-Performing Architectures - Scenario",
        "questionText": "A company has a read-heavy database workload running on Amazon RDS for PostgreSQL. Users are experiencing slow query performance during peak hours. Which strategy would be most effective for improving read performance with minimal changes to the application?",
        "options": [
            {
                "letter": "A",
                "text": "Increase the instance size of the primary RDS database."
            },
            {
                "letter": "B",
                "text": "Implement Amazon RDS Read Replicas."
            },
            {
                "letter": "C",
                "text": "Migrate the database to Amazon DynamoDB."
            },
            {
                "letter": "D",
                "text": "Enable Multi-AZ deployment for the RDS instance."
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **Implement Amazon RDS Read Replicas:** Read Replicas allow you to offload read traffic from your primary RDS database instance. You can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput. This is ideal for read-heavy workloads and typically requires minimal application changes (updating connection strings for read queries).\n\n**Why other options are incorrect:**\n\n*   **A) Increase the instance size of the primary RDS database:** While this can improve performance to some extent, it might not be the most scalable or cost-effective solution for read-heavy workloads compared to Read Replicas.\n*   **C) Migrate the database to Amazon DynamoDB:** This is a significant architectural change and involves migrating from a relational database to a NoSQL database, which would require substantial application modifications.\n*   **D) Enable Multi-AZ deployment for the RDS instance:** Multi-AZ provides high availability and data durability by creating a standby replica in a different AZ. It does not primarily serve read traffic or improve read performance for the primary instance (though you can read from the standby in some specific scenarios with certain database engines, it's not its main purpose for scaling reads)."
    },
    {
        "id": 20,
        "domain": "Design Cost-Optimized Architectures - Scenario",
        "questionText": "A company uses Amazon EC2 instances for a batch processing workload that runs overnight. The workload can be interrupted and resumed. They want to achieve the lowest possible compute cost. Which EC2 purchasing option is most suitable?",
        "options": [
            {
                "letter": "A",
                "text": "On-Demand Instances"
            },
            {
                "letter": "B",
                "text": "Standard Reserved Instances"
            },
            {
                "letter": "C",
                "text": "Spot Instances"
            },
            {
                "letter": "D",
                "text": "Convertible Reserved Instances"
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **Spot Instances:** Spot Instances offer the largest discounts on EC2 compute (up to 90% off On-Demand prices). They are ideal for fault-tolerant, flexible workloads like batch processing, data analysis, and background processing that can be interrupted. Since the workload can be interrupted and resumed, Spot Instances provide the best cost optimization.\n\n**Why other options are incorrect:**\n\n*   **A) On-Demand Instances:** These are more expensive than Spot Instances and are better suited for workloads that cannot tolerate interruptions or have unpredictable short-term needs.\n*   **B) Standard Reserved Instances:** These provide a discount for a 1 or 3-year commitment but are typically more expensive than Spot Instances. They are good for steady-state workloads.\n*   **D) Convertible Reserved Instances:** These also offer a discount for a commitment and provide flexibility to change instance attributes, but Spot Instances will still offer deeper discounts for interruptible workloads."
    },
    {
        "id": 21,
        "domain": "Design Secure Architectures - Scenario",
        "questionText": "A company needs to audit all API calls made to their AWS account for security analysis and compliance. Which AWS service should they use to record this information?",
        "options": [
            {
                "letter": "A",
                "text": "Amazon CloudWatch Logs"
            },
            {
                "letter": "B",
                "text": "AWS CloudTrail"
            },
            {
                "letter": "C",
                "text": "AWS Config"
            },
            {
                "letter": "D",
                "text": "Amazon Inspector"
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **AWS CloudTrail:** CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. It records API calls made on your account and delivers log files containing API call history to an S3 bucket. This information can be used for security analysis, resource change tracking, and troubleshooting.\n\n**Why other options are incorrect:**\n\n*   **A) Amazon CloudWatch Logs:** This service is used for monitoring, storing, and accessing log files from EC2 instances, AWS Lambda, and other sources. While it stores logs, CloudTrail is specifically designed for API call logging.\n*   **C) AWS Config:** This service assesses, audits, and evaluates the configurations of your AWS resources. It focuses on resource configuration changes rather than all API calls.\n*   **D) Amazon Inspector:** This is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. It checks for vulnerabilities and deviations from best practices, but it doesn't record all API calls."
    },
    {
        "id": 22,
        "domain": "Design Resilient Architectures - Scenario",
        "questionText": "A company has a stateless web application running on Amazon EC2 instances. They want to ensure that the application can automatically recover from individual instance failures. Which combination of AWS services/features would achieve this?",
        "options": [
            {
                "letter": "A",
                "text": "Amazon Route 53 health checks and manual instance replacement."
            },
            {
                "letter": "B",
                "text": "Elastic Load Balancer (ELB) with health checks and an Auto Scaling group."
            },
            {
                "letter": "C",
                "text": "AWS Lambda and Amazon API Gateway."
            },
            {
                "letter": "D",
                "text": "Amazon S3 for instance state backup and EC2 instance store volumes."
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **Elastic Load Balancer (ELB) with health checks and an Auto Scaling group:** An ELB distributes traffic across multiple instances and performs health checks. If an instance fails a health check, the ELB stops sending traffic to it. An Auto Scaling group can be configured to monitor the health of instances and automatically replace any unhealthy instances, maintaining the desired capacity and ensuring the application recovers from failures.\n\n**Why other options are incorrect:**\n\n*   **A) Amazon Route 53 health checks and manual instance replacement:** While Route 53 can perform health checks and reroute traffic, manual instance replacement is not an automatic recovery mechanism.\n*   **C) AWS Lambda and Amazon API Gateway:** This describes a serverless architecture, which is inherently resilient, but the question specifies EC2 instances.\n*   **D) Amazon S3 for instance state backup and EC2 instance store volumes:** Instance store volumes are ephemeral. While S3 can be used for backups, this combination doesn't provide automatic recovery from instance failures for a running application."
    },
    {
        "id": 23,
        "domain": "Design High-Performing Architectures - Scenario",
        "questionText": "A company wants to deploy a global application that serves dynamic content to users with the lowest possible latency. They also want to protect their application from common web exploits. Which AWS service combination is most suitable?",
        "options": [
            {
                "letter": "A",
                "text": "Amazon S3 with Cross-Region Replication and AWS Shield Standard."
            },
            {
                "letter": "B",
                "text": "Amazon CloudFront with AWS WAF and dynamic content acceleration."
            },
            {
                "letter": "C",
                "text": "Elastic Load Balancer with EC2 instances in multiple regions and Security Groups."
            },
            {
                "letter": "D",
                "text": "Amazon API Gateway with AWS Lambda and Amazon GuardDuty."
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **Amazon CloudFront with AWS WAF and dynamic content acceleration:** CloudFront is a global CDN that caches content close to users, reducing latency for both static and dynamic content (via features like dynamic content acceleration). AWS WAF (Web Application Firewall) integrates with CloudFront to protect against common web exploits like SQL injection and cross-site scripting.\n\n**Why other options are incorrect:**\n\n*   **A) Amazon S3 with Cross-Region Replication and AWS Shield Standard:** S3 is for object storage, primarily static content. While CloudFront can front S3, S3 itself doesn't serve dynamic content with low latency globally. AWS Shield Standard provides DDoS protection but doesn't address web exploits like WAF does.\n*   **C) Elastic Load Balancer with EC2 instances in multiple regions and Security Groups:** While this provides regional distribution, CloudFront offers a more comprehensive global edge network for lower latency. Security Groups provide network-level filtering, not application-layer protection like WAF.\n*   **D) Amazon API Gateway with AWS Lambda and Amazon GuardDuty:** This is a serverless backend. While API Gateway can use CloudFront edge-optimized endpoints, GuardDuty is a threat detection service, not a web application firewall."
    },
    {
        "id": 24,
        "domain": "Design Cost-Optimized Architectures - Scenario",
        "questionText": "A company is looking for a cost-effective way to run a small, simple web application that has predictable traffic. They prefer a solution with a fixed monthly cost that includes compute, storage, and networking. Which AWS service is designed for this requirement?",
        "options": [
            {
                "letter": "A",
                "text": "Amazon EC2 On-Demand instances"
            },
            {
                "letter": "B",
                "text": "AWS Elastic Beanstalk"
            },
            {
                "letter": "C",
                "text": "Amazon Lightsail"
            },
            {
                "letter": "D",
                "text": "AWS Fargate"
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **Amazon Lightsail:** Lightsail is designed to be the easiest way to launch and manage a virtual private server (VPS) with AWS. It offers simple, bundled pricing that includes a virtual machine, SSD-based storage, data transfer, DNS management, and a static IP address for a low, predictable monthly fee. This is ideal for small applications, websites, and dev/test environments where simplicity and predictable cost are key.\n\n**Why other options are incorrect:**\n\n*   **A) Amazon EC2 On-Demand instances:** While flexible, EC2 pricing is typically pay-as-you-go for compute, storage, and data transfer separately, which might not be a fixed monthly cost bundle.\n*   **B) AWS Elastic Beanstalk:** This is an orchestration service that deploys and manages applications on AWS infrastructure (like EC2, S3, ELB). You pay for the underlying resources, not a fixed bundle for Beanstalk itself.\n*   **D) AWS Fargate:** This is a serverless compute engine for containers. Pricing is based on vCPU and memory resources consumed by your containerized applications, which is pay-as-you-go, not a fixed monthly bundle."
    },
    {
        "id": 25,
        "domain": "Design Secure Architectures - Scenario",
        "questionText": "A company stores sensitive audit logs in an Amazon S3 bucket. To prevent accidental deletion or modification of these logs, they want to enforce Write-Once-Read-Many (WORM) protection. Which S3 feature should they use?",
        "options": [
            {
                "letter": "A",
                "text": "S3 Versioning"
            },
            {
                "letter": "B",
                "text": "S3 Object Lock"
            },
            {
                "letter": "C",
                "text": "S3 Bucket Policies restricting delete actions"
            },
            {
                "letter": "D",
                "text": "S3 Cross-Region Replication"
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **S3 Object Lock:** S3 Object Lock provides WORM protection for S3 objects. It can help you prevent objects from being deleted or overwritten for a fixed amount of time or indefinitely. You can use S3 Object Lock to meet regulatory requirements that require WORM storage, or simply as an added layer of protection against accidental changes.\n\n**Why other options are incorrect:**\n\n*   **A) S3 Versioning:** Versioning keeps multiple variants of an object in the same bucket. While it protects against accidental overwrites (by creating a new version) and deletions (by creating a delete marker), it doesn't prevent the deletion of specific versions or the delete markers themselves without additional controls, and it's not strictly WORM.\n*   **C) S3 Bucket Policies restricting delete actions:** While bucket policies can restrict delete actions, S3 Object Lock is specifically designed for WORM compliance and offers stronger, time-bound immutability.\n*   **D) S3 Cross-Region Replication:** This replicates objects to another bucket, providing data durability and availability, but it doesn't inherently provide WORM protection on the source or destination objects."
    },
    {
        "id": 26,
        "domain": "Design Resilient Architectures - Scenario",
        "questionText": "A company has an application that writes data to an Amazon S3 bucket. They need to ensure that the data is automatically replicated to another S3 bucket in a different AWS Region for disaster recovery purposes. The replication must happen as soon as objects are created. Which S3 feature should they configure?",
        "options": [
            {
                "letter": "A",
                "text": "S3 Versioning"
            },
            {
                "letter": "B",
                "text": "S3 Lifecycle Policies"
            },
            {
                "letter": "C",
                "text": "S3 Cross-Region Replication (CRR)"
            },
            {
                "letter": "D",
                "text": "S3 Same-Region Replication (SRR)"
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **S3 Cross-Region Replication (CRR):** CRR is an S3 feature that automatically replicates every object uploaded to a source S3 bucket to a destination bucket in a different AWS Region. This is ideal for disaster recovery, compliance, and reducing latency for users in different geographical locations.\n\n**Why other options are incorrect:**\n\n*   **A) S3 Versioning:** Versioning protects against accidental overwrites and deletions but doesn't replicate data to another region.\n*   **B) S3 Lifecycle Policies:** These are used to manage the lifecycle of objects, such as transitioning them to different storage classes or deleting them after a certain period. They don't replicate data.\n*   **D) S3 Same-Region Replication (SRR):** SRR replicates objects to another bucket within the *same* AWS Region. The requirement is for cross-region replication for disaster recovery."
    },
    {
        "id": 27,
        "domain": "Design High-Performing Architectures - Scenario",
        "questionText": "A company is migrating a legacy application to AWS. The application frequently reads and writes small, temporary files to a local disk. They need a high-performance storage option for these temporary files on their EC2 instances. Which storage option is most suitable?",
        "options": [
            {
                "letter": "A",
                "text": "Amazon EBS General Purpose SSD (gp3)"
            },
            {
                "letter": "B",
                "text": "Amazon EBS Provisioned IOPS SSD (io2 Block Express)"
            },
            {
                "letter": "C",
                "text": "EC2 Instance Store (NVMe SSD)"
            },
            {
                "letter": "D",
                "text": "Amazon S3 Standard"
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **EC2 Instance Store (NVMe SSD):** Instance stores provide temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer. Instance stores are ideal for temporary storage of information that changes frequently, such as buffers, caches, scratch data, and other temporary content, or for data that is replicated across a fleet of instances. NVMe SSD instance stores offer very high IOPS and low latency.\n\n**Why other options are incorrect:**\n\n*   **A) Amazon EBS General Purpose SSD (gp3):** While good for many workloads, instance stores typically offer higher IOPS and lower latency for temporary, high-performance needs because they are physically attached.\n*   **B) Amazon EBS Provisioned IOPS SSD (io2 Block Express):** This offers the highest performance EBS volumes but is generally more expensive and might be overkill if the data is truly temporary and doesn't need to persist beyond the instance lifecycle. Instance store can be more cost-effective for this specific use case.\n*   **D) Amazon S3 Standard:** S3 is object storage and not suitable for use as a local disk for frequent read/write of small, temporary files requiring block storage performance."
    },
    {
        "id": 28,
        "domain": "Design Cost-Optimized Architectures - Scenario",
        "questionText": "A company has a development environment running on several m5.large EC2 instances. These instances are only used during business hours, Monday to Friday. They are currently using On-Demand instances. What is the MOST cost-effective way to run these instances while ensuring they are available when needed?",
        "options": [
            {
                "letter": "A",
                "text": "Purchase Standard Reserved Instances for all instances."
            },
            {
                "letter": "B",
                "text": "Implement a schedule to stop the instances outside business hours and use On-Demand pricing."
            },
            {
                "letter": "C",
                "text": "Use Spot Instances for all development instances."
            },
            {
                "letter": "D",
                "text": "Purchase a Savings Plan covering the m5.large usage."
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **Implement a schedule to stop the instances outside business hours and use On-Demand pricing:** For instances that are only needed for a predictable portion of the day/week, stopping them when not in use is the most direct way to save costs. You only pay for the compute time when the instances are running. This can be automated using AWS Instance Scheduler or custom scripts.\n\n**Why other options are incorrect:**\n\n*   **A) Purchase Standard Reserved Instances for all instances:** RIs provide a discount for a 1 or 3-year commitment. If instances are off for more than ~30-40% of the time, the RI discount might not be as beneficial as simply stopping On-Demand instances.\n*   **C) Use Spot Instances for all development instances:** Spot Instances can be interrupted, which might not be ideal for a development environment where developers need consistent access during business hours, even if some interruption tolerance exists.\n*   **D) Purchase a Savings Plan covering the m5.large usage:** Similar to RIs, Savings Plans offer discounts for a commitment to a certain amount of compute usage. If the instances are stopped for significant periods, the committed spend might not be fully utilized effectively compared to just stopping On-Demand instances."
    },
    {
        "id": 29,
        "domain": "Design Secure Architectures - Scenario",
        "questionText": "A company wants to centrally manage and automate the patching of its Amazon EC2 instances for both operating system and application vulnerabilities. Which AWS service is best suited for this task?",
        "options": [
            {
                "letter": "A",
                "text": "AWS Systems Manager Patch Manager"
            },
            {
                "letter": "B",
                "text": "Amazon Inspector"
            },
            {
                "letter": "C",
                "text": "AWS Security Hub"
            },
            {
                "letter": "D",
                "text": "AWS Config"
            }
        ],
        "correctAnswerLetter": "A",
        "explanation": "*   **AWS Systems Manager Patch Manager:** Patch Manager, a capability of AWS Systems Manager, automates the process of patching managed instances with both security-related and other types of updates. You can use Patch Manager to apply patches for both operating systems and applications.\n\n**Why other options are incorrect:**\n\n*   **B) Amazon Inspector:** Inspector is a vulnerability management service that scans EC2 instances and container images for software vulnerabilities and unintended network exposure. It identifies vulnerabilities but doesn't perform the patching itself.\n*   **C) AWS Security Hub:** Security Hub provides a comprehensive view of your security alerts and security posture across your AWS accounts. It aggregates findings from various services but doesn't directly manage patching.\n*   **D) AWS Config:** AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. It can track configuration changes and ensure compliance but is not a patching tool."
    },
    {
        "id": 30,
        "domain": "Design Resilient Architectures - Scenario",
        "questionText": "A company is designing an application that requires a relational database. They need to ensure that the database can automatically failover to a standby replica in a different Availability Zone in case of an issue, with minimal data loss (low RPO) and quick recovery (low RTO). Which Amazon RDS deployment option should they choose?",
        "options": [
            {
                "letter": "A",
                "text": "RDS Single-AZ deployment with frequent snapshots."
            },
            {
                "letter": "B",
                "text": "RDS Multi-AZ deployment."
            },
            {
                "letter": "C",
                "text": "RDS Read Replica in a different AZ."
            },
            {
                "letter": "D",
                "text": "Manually provisioning a standby instance and configuring replication."
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **RDS Multi-AZ deployment:** When you provision a Multi-AZ DB instance, Amazon RDS automatically creates a primary DB instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). In the event of planned database maintenance, DB instance failure, or an AZ failure, Amazon RDS automatically performs a failover to the standby instance. This provides high availability, data durability, and automatic failover.\n\n**Why other options are incorrect:**\n\n*   **A) RDS Single-AZ deployment with frequent snapshots:** Snapshots are for backups and point-in-time recovery, not for automatic failover with low RTO.\n*   **C) RDS Read Replica in a different AZ:** Read Replicas are primarily for scaling read traffic and are asynchronously replicated. While a Read Replica can be promoted to a standalone instance in a DR scenario, it's a manual process and might involve more data loss than a Multi-AZ synchronous replication failover.\n*   **D) Manually provisioning a standby instance and configuring replication:** This would be complex to manage, less reliable, and defeat the purpose of using a managed service like RDS which offers automated solutions like Multi-AZ."
    },
    {
        "id": 31,
        "domain": "Design High-Performing Architectures - Scenario",
        "questionText": "A company has a global application with users in multiple continents. They want to provide low-latency access to both static and dynamic content. Which AWS service should be the primary component for achieving this global content delivery?",
        "options": [
            {
                "letter": "A",
                "text": "Amazon S3 Transfer Acceleration"
            },
            {
                "letter": "B",
                "text": "Elastic Load Balancing (Application Load Balancer)"
            },
            {
                "letter": "C",
                "text": "Amazon CloudFront"
            },
            {
                "letter": "D",
                "text": "AWS Global Accelerator"
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **Amazon CloudFront:** CloudFront is a content delivery network (CDN) that caches static and dynamic content at edge locations around the world, closer to users. This significantly reduces latency for content delivery. It supports custom origins, including S3 buckets, ELBs, or any HTTP server.\n\n**Why other options are incorrect:**\n\n*   **A) Amazon S3 Transfer Acceleration:** This speeds up transfers to and from S3 buckets over long distances but is not a full CDN solution for delivering content to end-users with low latency.\n*   **B) Elastic Load Balancing (Application Load Balancer):** ALBs operate within a region and distribute traffic to backend instances. While CloudFront can use an ALB as an origin, the ALB itself doesn't provide global edge caching.\n*   **D) AWS Global Accelerator:** Global Accelerator improves the availability and performance of your applications with local or global users. It directs traffic to optimal endpoints over the AWS global network. While it improves performance for application endpoints, CloudFront is more specifically a CDN for caching and delivering content from edge locations. Often, Global Accelerator and CloudFront can be used together for different aspects of application delivery."
    },
    {
        "id": 32,
        "domain": "Design Cost-Optimized Architectures - Scenario",
        "questionText": "A company wants to archive 100TB of data that is rarely accessed but must be retained for 10 years. Retrieval times of up to 12 hours are acceptable. Which storage solution offers the lowest cost for this long-term archival requirement?",
        "options": [
            {
                "letter": "A",
                "text": "Amazon S3 Standard"
            },
            {
                "letter": "B",
                "text": "Amazon S3 Glacier Flexible Retrieval"
            },
            {
                "letter": "C",
                "text": "Amazon S3 Glacier Deep Archive"
            },
            {
                "letter": "D",
                "text": "Amazon EBS Cold HDD (sc1) volumes"
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **Amazon S3 Glacier Deep Archive:** This S3 storage class is designed for long-term data archival where data is accessed very infrequently (e.g., once or twice a year) and retrieval times of 12-48 hours are acceptable. It offers the absolute lowest storage cost in AWS.\n\n**Why other options are incorrect:**\n\n*   **A) Amazon S3 Standard:** This is for frequently accessed data and would be prohibitively expensive for 100TB of archival data.\n*   **B) Amazon S3 Glacier Flexible Retrieval:** This is also for archival but is slightly more expensive than Deep Archive and offers faster retrieval options (minutes to hours). If 12-hour retrieval is acceptable, Deep Archive is more cost-effective.\n*   **D) Amazon EBS Cold HDD (sc1) volumes:** EBS volumes are block storage attached to EC2 instances. They are not suitable or cost-effective for long-term, large-scale data archival compared to S3 Glacier Deep Archive."
    },
    {
        "id": 33,
        "domain": "Design Secure Architectures - Scenario",
        "questionText": "A company needs to protect its web application, hosted on AWS, from common web exploits such as SQL injection and Cross-Site Scripting (XSS). Which AWS service should they implement?",
        "options": [
            {
                "letter": "A",
                "text": "AWS Shield Advanced"
            },
            {
                "letter": "B",
                "text": "AWS WAF (Web Application Firewall)"
            },
            {
                "letter": "C",
                "text": "Amazon GuardDuty"
            },
            {
                "letter": "D",
                "text": "Network Access Control Lists (NACLs)"
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **AWS WAF (Web Application Firewall):** AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources. You can define custom rules or use managed rule sets to block common attack patterns like SQL injection or XSS.\n\n**Why other options are incorrect:**\n\n*   **A) AWS Shield Advanced:** This is a managed Distributed Denial of Service (DDoS) protection service. While it protects against DDoS attacks, it doesn't specifically address application-layer exploits like SQL injection or XSS in the same way WAF does.\n*   **C) Amazon GuardDuty:** This is a threat detection service that continuously monitors for malicious activity and unauthorized behavior. It detects threats but doesn't actively block them like WAF.\n*   **D) Network Access Control Lists (NACLs):** NACLs are stateless firewalls that operate at the subnet level, controlling inbound and outbound traffic. They work at the network layer (IP address, port) and don't inspect application-layer traffic for exploits like WAF."
    },
    {
        "id": 34,
        "domain": "Design Resilient Architectures - Scenario",
        "questionText": "An application stores critical user session data in an Amazon ElastiCache for Redis cluster. To ensure high availability and minimize data loss in case of a node failure, what feature should be enabled for the Redis cluster?",
        "options": [
            {
                "letter": "A",
                "text": "Cluster Mode disabled"
            },
            {
                "letter": "B",
                "text": "Multi-AZ with Automatic Failover"
            },
            {
                "letter": "C",
                "text": "Read replicas in different regions"
            },
            {
                "letter": "D",
                "text": "S3 backup and restore"
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **Multi-AZ with Automatic Failover:** For ElastiCache for Redis, enabling Multi-AZ with Automatic Failover enhances high availability. If the primary node fails, ElastiCache automatically promotes one of the available read replicas (in a different AZ) to be the new primary, and updates the DNS endpoint. This minimizes downtime and data loss.\n\n**Why other options are incorrect:**\n\n*   **A) Cluster Mode disabled:** Running Redis in non-cluster mode means all data is on a single node (potentially with replicas). Cluster mode enabled allows sharding data across multiple primaries, which is a different scaling and availability pattern. For simple primary-replica failover, Multi-AZ is key.\n*   **C) Read replicas in different regions:** While read replicas can exist, cross-region replication for ElastiCache for Redis is not its primary HA mechanism within a region. Multi-AZ focuses on intra-region HA.\n*   **D) S3 backup and restore:** Backups are important for DR but restoring from a backup involves downtime and potential data loss up to the last backup. It's not a high availability solution for immediate node failure."
    },
    {
        "id": 35,
        "domain": "Design High-Performing Architectures - Scenario",
        "questionText": "A company is running a data processing application on EC2 instances that needs to frequently access a shared file system with high throughput and consistency. Multiple instances need to concurrently access and modify files in this shared storage. Which AWS storage service is most appropriate?",
        "options": [
            {
                "letter": "A",
                "text": "Amazon S3"
            },
            {
                "letter": "B",
                "text": "Amazon EBS (attached to one instance and shared via OS-level sharing)"
            },
            {
                "letter": "C",
                "text": "Amazon EFS (Elastic File System)"
            },
            {
                "letter": "D",
                "text": "AWS Storage Gateway (File Gateway)"
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **Amazon EFS (Elastic File System):** EFS provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. It is designed to provide massively parallel shared access for thousands of EC2 instances, enabling applications to scale and achieve high levels of aggregate throughput and IOPS with low latencies. It supports the NFSv4.1 protocol and provides strong consistency.\n\n**Why other options are incorrect:**\n\n*   **A) Amazon S3:** S3 is object storage. While it can be mounted as a file system using third-party tools or File Gateway, it doesn't offer the same POSIX file system semantics or strong consistency for concurrent read/write access that EFS provides directly.\n*   **B) Amazon EBS (attached to one instance and shared via OS-level sharing):** Standard EBS volumes can only be attached to a single EC2 instance at a time (except for EBS Multi-Attach for specific instance families and workloads, which has limitations). Sharing via OS-level NFS from one instance adds complexity and a single point of failure.\n*   **D) AWS Storage Gateway (File Gateway):** File Gateway provides on-premises access to data in S3 as a file share. It's not designed as a primary high-performance shared file system for EC2 instances within AWS."
    },
    {
        "id": 36,
        "domain": "Design Cost-Optimized Architectures - Scenario",
        "questionText": "A company has a workload with predictable compute requirements that will run continuously for the next 3 years. They want the maximum possible discount on their EC2 instance costs. Which EC2 purchasing option or plan should they choose?",
        "options": [
            {
                "letter": "A",
                "text": "On-Demand Instances"
            },
            {
                "letter": "B",
                "text": "Spot Instances"
            },
            {
                "letter": "C",
                "text": "Standard Reserved Instances (3-year term, All Upfront payment)"
            },
            {
                "letter": "D",
                "text": "Compute Savings Plan (1-year term, No Upfront payment)"
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **Standard Reserved Instances (3-year term, All Upfront payment):** For workloads with highly predictable, long-term usage, Standard Reserved Instances offer significant discounts compared to On-Demand pricing. A 3-year term provides a larger discount than a 1-year term, and paying All Upfront provides the largest discount within that term compared to Partial Upfront or No Upfront.\n\n**Why other options are incorrect:**\n\n*   **A) On-Demand Instances:** These offer no discount and are the most expensive for continuous long-term workloads.\n*   **B) Spot Instances:** While offering deep discounts, they can be interrupted and are not suitable for workloads that must run continuously without interruption.\n*   **D) Compute Savings Plan (1-year term, No Upfront payment):** Savings Plans offer flexibility and good discounts, but a 3-year RI with All Upfront payment will generally provide a higher percentage discount for a specific instance type commitment."
    },
    {
        "id": 37,
        "domain": "Design Secure Architectures - Scenario",
        "questionText": "A company needs to securely store and manage API keys, database credentials, and other secrets used by their applications running on AWS. They want a service that provides automatic rotation of credentials for supported AWS services. Which service should they use?",
        "options": [
            {
                "letter": "A",
                "text": "AWS IAM"
            },
            {
                "letter": "B",
                "text": "AWS Key Management Service (KMS)"
            },
            {
                "letter": "C",
                "text": "AWS Secrets Manager"
            },
            {
                "letter": "D",
                "text": "Amazon S3 with server-side encryption"
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **AWS Secrets Manager:** This service helps you protect secrets needed to access your applications, services, and IT resources. Secrets Manager enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. It offers built-in integration for rotating credentials for Amazon RDS, Amazon Redshift, and Amazon DocumentDB.\n\n**Why other options are incorrect:**\n\n*   **A) AWS IAM:** IAM is for managing access to AWS services and resources (users, roles, permissions). It doesn't store or rotate application secrets like database passwords.\n*   **B) AWS Key Management Service (KMS):** KMS is used to create and manage encryption keys and control their use across AWS services and applications. While Secrets Manager uses KMS to encrypt secrets, KMS itself is not the secret storage and rotation service.\n*   **D) Amazon S3 with server-side encryption:** S3 can store files securely, but it's not a dedicated secrets management service with features like automatic rotation and integration with applications for retrieving secrets."
    },
    {
        "id": 38,
        "domain": "Design Resilient Architectures - Scenario",
        "questionText": "A company is deploying a web application that requires a Network Load Balancer (NLB). They need to ensure that the NLB itself is highly available and can withstand the failure of an entire Availability Zone. How does an NLB achieve this?",
        "options": [
            {
                "letter": "A",
                "text": "By deploying NLB nodes in a single, highly resilient AZ."
            },
            {
                "letter": "B",
                "text": "By requiring users to configure NLB nodes in multiple AZs manually."
            },
            {
                "letter": "C",
                "text": "By automatically distributing NLB capacity across multiple enabled Availability Zones in a region."
            },
            {
                "letter": "D",
                "text": "By using AWS Global Accelerator to route traffic to healthy NLB nodes."
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **By automatically distributing NLB capacity across multiple enabled Availability Zones in a region:** Network Load Balancers are designed to be highly available. When you enable multiple Availability Zones for your NLB, AWS automatically provisions NLB nodes in those AZs. If one AZ becomes unavailable, the NLB nodes in the other AZs continue to handle traffic, ensuring the load balancer itself remains operational.\n\n**Why other options are incorrect:**\n\n*   **A) By deploying NLB nodes in a single, highly resilient AZ:** This would still be a single point of failure at the AZ level.\n*   **B) By requiring users to configure NLB nodes in multiple AZs manually:** AWS manages the provisioning and HA of the NLB nodes automatically once AZs are enabled for the NLB.\n*   **D) By using AWS Global Accelerator to route traffic to healthy NLB nodes:** While Global Accelerator can enhance performance and availability for applications fronted by NLBs, the intrinsic HA of the NLB itself comes from its multi-AZ design within the region."
    },
    {
        "id": 39,
        "domain": "Design High-Performing Architectures - Scenario",
        "questionText": "A company has a large dataset in Amazon S3 and needs to run complex analytical queries directly on this data without loading it into a traditional data warehouse. The queries need to be interactive and return results quickly. Which AWS service is best suited for this purpose?",
        "options": [
            {
                "letter": "A",
                "text": "Amazon RDS"
            },
            {
                "letter": "B",
                "text": "Amazon Redshift Spectrum"
            },
            {
                "letter": "C",
                "text": "Amazon DynamoDB with DAX"
            },
            {
                "letter": "D",
                "text": "Amazon ElastiCache"
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **Amazon Redshift Spectrum:** Redshift Spectrum is a feature of Amazon Redshift that allows you to run SQL queries directly against exabytes of data in Amazon S3. It enables you to query your S3 data lake without needing to load or transform the data. This is ideal for interactive, complex analytical queries on large datasets in S3.\n\n**Why other options are incorrect:**\n\n*   **A) Amazon RDS:** RDS is a relational database service, not designed for querying massive datasets directly in S3 for analytics.\n*   **C) Amazon DynamoDB with DAX:** DynamoDB is a NoSQL database, and DAX is its in-memory cache. Neither is suited for complex SQL-based analytical queries on S3 data.\n*   **D) Amazon ElastiCache:** This is an in-memory caching service, not a query engine for S3 data."
    },
    {
        "id": 40,
        "domain": "Design Cost-Optimized Architectures - Scenario",
        "questionText": "A company is using Amazon EC2 On-Demand instances for a workload that runs 24/7. They have a stable and predictable usage pattern for these instances over the next year. They want to reduce costs but also need the flexibility to change instance families, sizes, or operating systems if their needs evolve. Which purchasing option offers a good balance of discount and flexibility?",
        "options": [
            {
                "letter": "A",
                "text": "Standard Reserved Instances"
            },
            {
                "letter": "B",
                "text": "Convertible Reserved Instances"
            },
            {
                "letter": "C",
                "text": "Spot Instances"
            },
            {
                "letter": "D",
                "text": "No upfront EC2 Instance Savings Plans"
            }
        ],
        "correctAnswerLetter": "D",
        "explanation": "*   **No upfront EC2 Instance Savings Plans:** Savings Plans provide significant discounts (similar to RIs) in exchange for a commitment to a consistent amount of compute usage (measured in $/hour) for a 1 or 3-year term. EC2 Instance Savings Plans offer the most flexibility, automatically applying to usage across instance families, sizes, OS, tenancy, and regions (within the plan type). A \n\nNo Upfront payment option provides the discount without requiring an initial payment, making it easier to adopt.\n\n**Why other options are incorrect:**\n\n*   **A) Standard Reserved Instances:** These offer good discounts but lock you into a specific instance family, size, OS, and region, providing less flexibility than Savings Plans.\n*   **B) Convertible Reserved Instances:** These offer more flexibility than Standard RIs (allowing changes to instance family, OS, etc.) but typically provide a slightly lower discount than Standard RIs or Savings Plans for the same commitment term.\n*   **C) Spot Instances:** These are not suitable for workloads that must run 24/7 without interruption."
    },
    {
        "id": 41,
        "domain": "Design Secure Architectures - Scenario",
        "questionText": "A company wants to ensure that all data stored in their Amazon S3 buckets is encrypted at rest by default, without requiring users to specify encryption settings for each object upload. How can they achieve this?",
        "options": [
            {
                "letter": "A",
                "text": "Enable S3 Versioning on all buckets."
            },
            {
                "letter": "B",
                "text": "Configure default encryption on the S3 buckets using SSE-S3 or SSE-KMS."
            },
            {
                "letter": "C",
                "text": "Use client-side encryption for all object uploads."
            },
            {
                "letter": "D",
                "text": "Implement an IAM policy that denies uploads without encryption headers."
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **Configure default encryption on the S3 buckets using SSE-S3 or SSE-KMS:** Amazon S3 supports default bucket encryption. When you enable default encryption for a bucket, all new objects are automatically encrypted when they are stored in the bucket. You can choose between Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3) or Server-Side Encryption with AWS Key Management Service (SSE-KMS).\n\n**Why other options are incorrect:**\n\n*   **A) Enable S3 Versioning on all buckets:** Versioning protects objects from overwrites and deletions but doesn't enforce encryption.\n*   **C) Use client-side encryption for all object uploads:** While client-side encryption is a valid approach, the requirement is for default server-side encryption without user intervention for each upload.\n*   **D) Implement an IAM policy that denies uploads without encryption headers:** While an IAM policy can enforce that uploads *include* encryption headers, setting default bucket encryption is a more direct and simpler way to ensure all objects are encrypted at rest automatically."
    },
    {
        "id": 42,
        "domain": "Design Resilient Architectures - Scenario",
        "questionText": "A company runs a critical application on Amazon EC2 instances. They need to back up the instance data (stored on EBS volumes) regularly and be able to restore it to a specific point in time in case of data corruption. Which AWS service or feature is most suitable for this?",
        "options": [
            {
                "letter": "A",
                "text": "Amazon S3 Cross-Region Replication"
            },
            {
                "letter": "B",
                "text": "AWS Backup or EBS Snapshots"
            },
            {
                "letter": "C",
                "text": "EC2 Instance Store"
            },
            {
                "letter": "D",
                "text": "AWS Storage Gateway Volume Gateway (Cached Volumes)"
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **AWS Backup or EBS Snapshots:** Amazon EBS snapshots are point-in-time copies of your EBS volumes, stored in S3. You can use them to back up your data and restore it to new EBS volumes. AWS Backup is a fully managed backup service that makes it easy to centralize and automate data protection across AWS services, including EBS volumes. It allows you to create backup plans, define retention policies, and perform point-in-time restores.\n\n**Why other options are incorrect:**\n\n*   **A) Amazon S3 Cross-Region Replication:** This is for replicating S3 objects, not backing up EBS volumes.\n*   **C) EC2 Instance Store:** Instance store is ephemeral storage and not suitable for persistent data backup.\n*   **D) AWS Storage Gateway Volume Gateway (Cached Volumes):** While Volume Gateway provides cloud-backed storage volumes, EBS snapshots or AWS Backup are the direct and recommended methods for backing up EBS volumes within AWS for point-in-time recovery."
    },
    {
        "id": 43,
        "domain": "Design High-Performing Architectures - Scenario",
        "questionText": "A company is developing a real-time bidding application that requires extremely low latency (sub-millisecond) for read and write operations to a key-value datastore. The dataset is relatively small but accessed very frequently. Which AWS service combination would provide the best performance?",
        "options": [
            {
                "letter": "A",
                "text": "Amazon RDS for MySQL with Read Replicas."
            },
            {
                "letter": "B",
                "text": "Amazon DynamoDB with On-Demand capacity."
            },
            {
                "letter": "C",
                "text": "Amazon ElastiCache for Redis."
            },
            {
                "letter": "D",
                "text": "Amazon S3 with CloudFront for caching."
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **Amazon ElastiCache for Redis:** ElastiCache for Redis is an in-memory data store that delivers microsecond latency. It is commonly used for caching, session management, real-time analytics, and leaderboards, making it ideal for applications requiring extremely low latency like real-time bidding.\n\n**Why other options are incorrect:**\n\n*   **A) Amazon RDS for MySQL with Read Replicas:** Relational databases, even with read replicas, typically have higher latency than in-memory stores like Redis, especially for write operations.\n*   **B) Amazon DynamoDB with On-Demand capacity:** DynamoDB offers single-digit millisecond latency, which is very good, but ElastiCache for Redis can achieve even lower (microsecond) latency for in-memory operations.\n*   **D) Amazon S3 with CloudFront for caching:** S3 is object storage, and CloudFront is a CDN. This combination is for content delivery, not a low-latency key-value datastore for frequent reads and writes."
    },
    {
        "id": 44,
        "domain": "Design Cost-Optimized Architectures - Scenario",
        "questionText": "A company has a large number of EC2 instances that are part of an Auto Scaling group. The workload is highly variable, and they want to optimize costs by using the most economical EC2 instances that meet their performance requirements. They are willing to tolerate interruptions for some parts of their workload. How can they configure their Auto Scaling group to achieve this?",
        "options": [
            {
                "letter": "A",
                "text": "Use only On-Demand Instances in the Auto Scaling group."
            },
            {
                "letter": "B",
                "text": "Use a launch template that specifies only the largest instance types."
            },
            {
                "letter": "C",
                "text": "Configure the Auto Scaling group to use a mix of On-Demand Instances and Spot Instances across multiple instance types."
            },
            {
                "letter": "D",
                "text": "Purchase Reserved Instances for the maximum capacity of the Auto Scaling group."
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **Configure the Auto Scaling group to use a mix of On-Demand Instances and Spot Instances across multiple instance types:** Auto Scaling groups can be configured to launch instances from multiple instance types and purchasing options (On-Demand and Spot). By using Spot Instances for a portion of the capacity, especially for fault-tolerant workloads, significant cost savings can be achieved. Diversifying across multiple instance types (Spot Instance pools) increases the likelihood of obtaining Spot capacity.\n\n**Why other options are incorrect:**\n\n*   **A) Use only On-Demand Instances in the Auto Scaling group:** This does not leverage the cost savings potential of Spot Instances.\n*   **B) Use a launch template that specifies only the largest instance types:** This is not a cost optimization strategy and might lead to over-provisioning.\n*   **D) Purchase Reserved Instances for the maximum capacity of the Auto Scaling group:** This can lead to underutilized RIs if the workload is highly variable and doesn't consistently use the maximum capacity."
    },
    {
        "id": 45,
        "domain": "Design Secure Architectures - Scenario",
        "questionText": "A company needs to provide fine-grained access control to specific objects within an Amazon S3 bucket for different users and applications. Some objects should be public, while others should be private or accessible only to specific IAM roles. Which S3 security feature is most appropriate for managing access at the object level?",
        "options": [
            {
                "letter": "A",
                "text": "S3 Bucket Policies"
            },
            {
                "letter": "B",
                "text": "S3 Access Control Lists (ACLs)"
            },
            {
                "letter": "C",
                "text": "IAM User Policies"
            },
            {
                "letter": "D",
                "text": "S3 Default Encryption"
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **S3 Access Control Lists (ACLs):** ACLs are a legacy access control mechanism that can be used to grant basic read/write permissions to other AWS accounts or to the public at the individual object level. While bucket policies and IAM policies are generally recommended for managing permissions, ACLs are still used for fine-grained object-level control, especially when needing to grant access to specific objects to different entities or make individual objects public. Note: AWS recommends using S3 bucket policies and IAM policies for most access control use cases, but ACLs are the direct answer for object-level granularity when different objects in the same bucket need different permissions for external accounts or public access.\n\n**Why other options are incorrect:**\n\n*   **A) S3 Bucket Policies:** Bucket policies apply to all objects within a bucket or to objects matching a prefix. While powerful, they are not as granular as object ACLs for defining permissions on a per-object basis for different external entities.\n*   **C) IAM User Policies:** IAM policies define permissions for IAM users, groups, or roles, typically granting access to buckets or prefixes, not usually individual, disparate objects with varying permissions within a single bucket for multiple external entities.\n*   **D) S3 Default Encryption:** This feature encrypts objects at rest but does not control access to them."
    },
    {
        "id": 46,
        "domain": "Design Resilient Architectures - Scenario",
        "questionText": "A company is deploying an application that uses Amazon Simple Queue Service (SQS) to decouple components. They need to ensure that messages are processed successfully at least once, even if the consumer instance fails during processing. Which SQS feature helps achieve this?",
        "options": [
            {
                "letter": "A",
                "text": "SQS Short Polling"
            },
            {
                "letter": "B",
                "text": "SQS Dead-Letter Queues (DLQs)"
            },
            {
                "letter": "C",
                "text": "SQS Visibility Timeout"
            },
            {
                "letter": "D",
                "text": "SQS FIFO Queues"
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **SQS Visibility Timeout:** When a consumer receives a message from an SQS queue, the message remains in the queue but is hidden for a configurable period called the visibility timeout. If the consumer processes the message successfully, it deletes the message from the queue. If the consumer fails before deleting the message (and the visibility timeout expires), the message becomes visible again for another consumer to process. This ensures that messages are not lost if a consumer fails.\n\n**Why other options are incorrect:**\n\n*   **A) SQS Short Polling:** This refers to how consumers request messages (returning immediately, even if the queue is empty). Long polling is generally preferred for reducing empty responses and cost.\n*   **B) SQS Dead-Letter Queues (DLQs):** DLQs are used to store messages that couldn't be processed successfully after a certain number of attempts. While important for handling problematic messages, the visibility timeout is the primary mechanism for ensuring at-least-once processing in case of consumer failure.\n*   **D) SQS FIFO Queues:** FIFO queues provide exactly-once processing and preserve message order, but the core mechanism for handling consumer failures and re-processing is still related to the visibility timeout concept."
    },
    {
        "id": 47,
        "domain": "Design High-Performing Architectures - Scenario",
        "questionText": "A company needs to connect its on-premises data center to its AWS VPC with a dedicated, private connection that offers consistent network performance and high bandwidth (e.g., 1 Gbps or 10 Gbps). Which AWS service should they use?",
        "options": [
            {
                "letter": "A",
                "text": "AWS Site-to-Site VPN"
            },
            {
                "letter": "B",
                "text": "VPC Peering"
            },
            {
                "letter": "C",
                "text": "AWS Direct Connect"
            },
            {
                "letter": "D",
                "text": "AWS Transit Gateway"
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **AWS Direct Connect:** This service establishes a dedicated private network connection from your on-premises data center to AWS. Direct Connect can reduce network costs, increase bandwidth throughput, and provide a more consistent network experience than internet-based connections like VPN. It offers port speeds of 1 Gbps, 10 Gbps, and 100 Gbps.\n\n**Why other options are incorrect:**\n\n*   **A) AWS Site-to-Site VPN:** VPN connections run over the public internet and may not offer the same level of consistent performance or high bandwidth as Direct Connect.\n*   **B) VPC Peering:** This connects two VPCs within AWS, not an on-premises data center to a VPC.\n*   **D) AWS Transit Gateway:** Transit Gateway acts as a network hub to connect multiple VPCs and on-premises networks. While Direct Connect can connect to a Transit Gateway, Direct Connect itself is the service providing the dedicated physical connection."
    },
    {
        "id": 48,
        "domain": "Design Cost-Optimized Architectures - Scenario",
        "questionText": "A company is using Amazon RDS for a database that has fluctuating and unpredictable read traffic. They want to ensure good read performance without over-provisioning the primary database instance or manually managing read replicas. Which RDS feature can help automate read scaling cost-effectively?",
        "options": [
            {
                "letter": "A",
                "text": "RDS Multi-AZ deployment"
            },
            {
                "letter": "B",
                "text": "Manually creating and deleting Read Replicas based on load."
            },
            {
                "letter": "C",
                "text": "Amazon Aurora Serverless v2"
            },
            {
                "letter": "D",
                "text": "Increasing the storage size of the primary RDS instance."
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **Amazon Aurora Serverless v2:** Aurora Serverless v2 is an on-demand, auto-scaling configuration for Amazon Aurora. It automatically starts up, shuts down, and scales capacity up or down based on your application's needs. For read traffic, it can automatically scale read capacity by adding or removing reader instances, providing cost-effective scaling for unpredictable workloads without manual intervention. (Note: While the question mentions RDS generally, Aurora is part of the RDS family and Aurora Serverless is a strong fit for this scenario).\n\n**Why other options are incorrect:**\n\n*   **A) RDS Multi-AZ deployment:** This is for high availability, not for scaling read traffic.\n*   **B) Manually creating and deleting Read Replicas based on load:** This is not automated and can be operationally intensive.\n*   **D) Increasing the storage size of the primary RDS instance:** Storage size is not directly related to scaling read compute capacity."
    },
    {
        "id": 49,
        "domain": "Design Secure Architectures - Scenario",
        "questionText": "A company wants to detect potential security threats and malicious activity within their AWS environment, such as compromised EC2 instances, unusual API calls, or data exfiltration attempts from S3. Which AWS service provides intelligent threat detection for this purpose?",
        "options": [
            {
                "letter": "A",
                "text": "AWS WAF"
            },
            {
                "letter": "B",
                "text": "Amazon Inspector"
            },
            {
                "letter": "C",
                "text": "Amazon GuardDuty"
            },
            {
                "letter": "D",
                "text": "AWS Shield"
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **Amazon GuardDuty:** GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3. It uses machine learning, anomaly detection, and integrated threat intelligence to identify and prioritize potential threats.\n\n**Why other options are incorrect:**\n\n*   **A) AWS WAF:** WAF protects web applications from common web exploits at the application layer.\n*   **B) Amazon Inspector:** Inspector assesses applications for vulnerabilities and deviations from best practices.\n*   **D) AWS Shield:** Shield is a managed DDoS protection service."
    },
    {
        "id": 50,
        "domain": "Design Resilient Architectures - Scenario",
        "questionText": "A company has a critical application that requires a Recovery Time Objective (RTO) of 15 minutes and a Recovery Point Objective (RPO) of 1 hour for its database. The database is currently running on a self-managed Oracle server on an EC2 instance. They are considering migrating to Amazon RDS. Which RDS backup and recovery strategy would best meet these RTO/RPO requirements?",
        "options": [
            {
                "letter": "A",
                "text": "Daily automated snapshots with manual restore."
            },
            {
                "letter": "B",
                "text": "RDS Multi-AZ deployment with automated snapshots enabled and point-in-time recovery."
            },
            {
                "letter": "C",
                "text": "Using only Amazon S3 to store manual database backups taken every 4 hours."
            },
            {
                "letter": "D",
                "text": "Replicating data to another EC2 instance in a different region using custom scripts."
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **RDS Multi-AZ deployment with automated snapshots enabled and point-in-time recovery:** RDS Multi-AZ provides an RTO typically within minutes by failing over to a standby instance. Automated snapshots combined with transaction logs enable point-in-time recovery (PITR), allowing you to restore to any second within your backup retention period, easily meeting an RPO of 1 hour (and often much lower). This combination offers both high availability and robust DR capabilities.\n\n**Why other options are incorrect:**\n\n*   **A) Daily automated snapshots with manual restore:** A daily snapshot means an RPO of up to 24 hours. Manual restore can take longer than 15 minutes RTO.\n*   **C) Using only Amazon S3 to store manual database backups taken every 4 hours:** An RPO of 4 hours doesn't meet the 1-hour requirement. Manual backups and restores are also less reliable and slower for meeting RTO.\n*   **D) Replicating data to another EC2 instance in a different region using custom scripts:** This is complex to manage, error-prone, and likely harder to achieve the stated RPO/RTO consistently compared to managed RDS features."
    },
    {
        "id": 51,
        "domain": "Design High-Performing Architectures - Scenario",
        "questionText": "A company is deploying a latency-sensitive application that requires a database with in-memory caching capabilities to accelerate query performance. The application primarily performs read operations. Which Amazon RDS option or feature would be most suitable?",
        "options": [
            {
                "letter": "A",
                "text": "Amazon RDS for SQL Server with Always On Availability Groups."
            },
            {
                "letter": "B",
                "text": "Amazon Aurora with a large instance class."
            },
            {
                "letter": "C",
                "text": "Amazon RDS for MySQL or PostgreSQL with an Amazon ElastiCache for Redis cluster integrated as a cache."
            },
            {
                "letter": "D",
                "text": "Amazon RDS for Oracle with Multi-AZ deployment."
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **Amazon RDS for MySQL or PostgreSQL with an Amazon ElastiCache for Redis cluster integrated as a cache:** For read-heavy, latency-sensitive applications, offloading reads to an in-memory cache like ElastiCache for Redis can significantly improve performance. Redis provides sub-millisecond latency for cached data. This approach allows the relational database to handle writes and cache misses while the cache serves frequent reads.\n\n**Why other options are incorrect:**\n\n*   **A) Amazon RDS for SQL Server with Always On Availability Groups:** This is primarily for high availability and disaster recovery for SQL Server, not specifically for in-memory caching to accelerate reads in the way ElastiCache does.\n*   **B) Amazon Aurora with a large instance class:** While Aurora is high-performing and larger instances have more memory which can be used for database caching, integrating a dedicated in-memory cache like ElastiCache is often more effective and scalable for extreme low-latency read requirements.\n*   **D) Amazon RDS for Oracle with Multi-AZ deployment:** Multi-AZ is for high availability, not primarily for read performance acceleration through in-memory caching."
    },
    {
        "id": 52,
        "domain": "Design Cost-Optimized Architectures - Scenario",
        "questionText": "A company runs a fleet of EC2 instances for various workloads. They want to get a consolidated view of their AWS costs and usage, identify cost-saving opportunities, and set budgets. Which AWS service provides these capabilities?",
        "options": [
            {
                "letter": "A",
                "text": "AWS Budgets"
            },
            {
                "letter": "B",
                "text": "Amazon CloudWatch"
            },
            {
                "letter": "C",
                "text": "AWS Cost Explorer"
            },
            {
                "letter": "D",
                "text": "AWS Trusted Advisor"
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **AWS Cost Explorer:** This service has an easy-to-use interface that lets you visualize, understand, and manage your AWS costs and usage over time. You can create custom reports, analyze your data at a high level (e.g., total costs and usage across all accounts) or dive deeper into your cost and usage data to identify trends, pinpoint cost drivers, and detect anomalies. It also provides recommendations for cost optimization.\n\n**Why other options are incorrect:**\n\n*   **A) AWS Budgets:** This service allows you to set custom budgets to track your cost and usage from the simplest to the most complex use cases. While related to cost management, Cost Explorer is the primary tool for visualization, analysis, and identifying saving opportunities.\n*   **B) Amazon CloudWatch:** CloudWatch is a monitoring and observability service. While it can track billing alarms via AWS Budgets, it's not the primary cost analysis tool.\n*   **D) AWS Trusted Advisor:** Trusted Advisor provides recommendations to help you optimize your AWS infrastructure, improve security and performance, reduce costs, and monitor service quotas. While it offers cost optimization checks, Cost Explorer is the dedicated tool for detailed cost and usage analysis."
    },
    {
        "id": 53,
        "domain": "Design Secure Architectures - Scenario",
        "questionText": "A company needs to ensure that all network traffic between their VPCs and their on-premises network, connected via AWS Direct Connect, is encrypted. Direct Connect itself does not encrypt traffic in transit. What is the recommended approach to encrypt this traffic?",
        "options": [
            {
                "letter": "A",
                "text": "Use AWS WAF to encrypt traffic over Direct Connect."
            },
            {
                "letter": "B",
                "text": "Establish an AWS Site-to-Site VPN connection over the AWS Direct Connect link."
            },
            {
                "letter": "C",
                "text": "Enable SSL/TLS on all applications communicating over Direct Connect."
            },
            {
                "letter": "D",
                "text": "Use Amazon Macie to encrypt sensitive data traversing Direct Connect."
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **Establish an AWS Site-to-Site VPN connection over the AWS Direct Connect link:** AWS Direct Connect provides a dedicated network path but does not inherently encrypt the traffic. To encrypt data in transit over a Direct Connect connection, you can establish an IPsec VPN (AWS Site-to-Site VPN) tunnel that runs over your Direct Connect private virtual interface (VIF). This combines the dedicated bandwidth of Direct Connect with the security of an encrypted VPN tunnel.\n\n**Why other options are incorrect:**\n\n*   **A) Use AWS WAF to encrypt traffic over Direct Connect:** WAF is a web application firewall and does not encrypt general network traffic over Direct Connect.\n*   **C) Enable SSL/TLS on all applications communicating over Direct Connect:** While application-level encryption (SSL/TLS) is a good practice, it doesn't encrypt all network traffic at the IP layer. An IPsec VPN provides broader network-level encryption.\n*   **D) Use Amazon Macie to encrypt sensitive data traversing Direct Connect:** Macie is a data security and data privacy service that uses machine learning and pattern matching to discover and protect your sensitive data in AWS. It does not encrypt traffic in transit."
    },
    {
        "id": 54,
        "domain": "Design Resilient Architectures - Scenario",
        "questionText": "A company has a stateless web application deployed on EC2 instances managed by an Auto Scaling group. They want to perform deployments with minimal downtime and have the ability to quickly roll back if issues occur. Which deployment strategy is most suitable for this?",
        "options": [
            {
                "letter": "A",
                "text": "All-at-once deployment."
            },
            {
                "letter": "B",
                "text": "Rolling deployment."
            },
            {
                "letter": "C",
                "text": "Blue/green deployment."
            },
            {
                "letter": "D",
                "text": "Canary deployment."
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **Blue/green deployment:** This strategy involves creating a new environment (green) identical to the production environment (blue). The new version of the application is deployed to the green environment. Once testing is complete, traffic is switched from the blue environment to the green environment. This allows for near-zero downtime deployments and provides a quick rollback path by simply switching traffic back to the blue environment if issues arise.\n\n**Why other options are incorrect:**\n\n*   **A) All-at-once deployment:** This replaces all existing instances simultaneously, which can cause downtime and offers no easy rollback.\n*   **B) Rolling deployment:** This updates a subset of instances at a time. While it reduces downtime compared to all-at-once, rollback can be more complex than blue/green.\n*   **D) Canary deployment:** This releases the new version to a small subset of users/servers first, then gradually rolls it out. While good for testing, blue/green is often preferred for its simpler rollback mechanism for entire environment switches."
    },
    {
        "id": 55,
        "domain": "Design High-Performing Architectures - Scenario",
        "questionText": "A company needs to process a massive stream of real-time data (e.g., clickstream data, IoT sensor data) with low latency. The processed data needs to be stored for further analysis. Which AWS service is best suited for ingesting and processing this real-time data stream?",
        "options": [
            {
                "letter": "A",
                "text": "Amazon SQS"
            },
            {
                "letter": "B",
                "text": "Amazon Kinesis Data Streams"
            },
            {
                "letter": "C",
                "text": "AWS Batch"
            },
            {
                "letter": "D",
                "text": "Amazon S3"
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **Amazon Kinesis Data Streams:** This service is designed for real-time ingestion and processing of streaming data at massive scale. It can continuously capture gigabytes of data per second from hundreds of thousands of sources. Multiple applications can consume and process the data concurrently from a Kinesis data stream.\n\n**Why other options are incorrect:**\n\n*   **A) Amazon SQS:** SQS is a message queuing service, suitable for decoupling applications and handling asynchronous tasks. While it can handle high throughput, Kinesis is specifically optimized for real-time streaming data ingestion and processing.\n*   **B) AWS Batch:** AWS Batch is for running batch computing workloads. It's not designed for real-time stream processing.\n*   **D) Amazon S3:** S3 is object storage. While processed data can be stored in S3, S3 itself is not a stream ingestion or processing service."
    },
    {
        "id": 56,
        "domain": "Design Cost-Optimized Architectures - Scenario",
        "questionText": "A company has several S3 buckets containing data with varying access patterns. Some data is accessed frequently, some infrequently, and some is archival. They want to automatically optimize storage costs without manually moving data between S3 storage classes. Which S3 feature should they use?",
        "options": [
            {
                "letter": "A",
                "text": "S3 Versioning"
            },
            {
                "letter": "B",
                "text": "S3 Lifecycle Policies with manual configuration for each access pattern."
            },
            {
                "letter": "C",
                "text": "S3 Intelligent-Tiering"
            },
            {
                "letter": "D",
                "text": "S3 Cross-Region Replication"
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **S3 Intelligent-Tiering:** This S3 storage class is designed to optimize storage costs by automatically moving data to the most cost-effective access tier when access patterns change, without performance impact or operational overhead. It monitors access patterns and moves objects that have not been accessed for a certain period to lower-cost infrequent access tiers.\n\n**Why other options are incorrect:**\n\n*   **A) S3 Versioning:** This protects data from accidental deletion or overwrites but doesn't optimize storage costs based on access patterns.\n*   **B) S3 Lifecycle Policies with manual configuration for each access pattern:** While lifecycle policies can transition objects between storage classes, S3 Intelligent-Tiering automates this process based on actual access patterns, requiring less manual configuration and potentially providing better optimization for unknown or changing patterns.\n*   **D) S3 Cross-Region Replication:** This replicates data to another region for DR or latency reduction, but it doesn't manage storage class tiering for cost optimization within a bucket."
    },
    {
        "id": 57,
        "domain": "Design Secure Architectures - Scenario",
        "questionText": "A company wants to provide secure, temporary access to its AWS resources for users federated from an external identity provider (IdP) that supports SAML 2.0. Which AWS service should be configured to enable this federated access?",
        "options": [
            {
                "letter": "A",
                "text": "Amazon Cognito User Pools"
            },
            {
                "letter": "B",
                "text": "AWS IAM Roles with a SAML 2.0 identity provider configured in IAM."
            },
            {
                "letter": "C",
                "text": "AWS Directory Service for Microsoft Active Directory."
            },
            {
                "letter": "D",
                "text": "AWS Single Sign-On (AWS SSO) connected directly to the external IdP."
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **AWS IAM Roles with a SAML 2.0 identity provider configured in IAM:** AWS IAM supports identity federation with SAML 2.0, allowing you to create trust relationships between your SAML-compatible IdP (like Active Directory Federation Services, Shibboleth, etc.) and AWS. Users can authenticate with their existing IdP, and then assume an IAM role that grants them temporary security credentials to access AWS resources.\n\n**Why other options are incorrect:**\n\n*   **A) Amazon Cognito User Pools:** While Cognito supports SAML federation for application users, for direct AWS resource access via federation, setting up a SAML IdP in IAM is the more direct approach.\n*   **C) AWS Directory Service for Microsoft Active Directory:** This is for running Active Directory in AWS or connecting to an on-premises AD. While it can be an IdP, the core AWS mechanism for SAML federation involves IAM.\n*   **D) AWS Single Sign-On (AWS SSO) connected directly to the external IdP:** AWS SSO can indeed connect to an external SAML 2.0 IdP and is a recommended way to manage access to multiple AWS accounts. It uses IAM roles under the hood. This is also a very strong answer. However, the fundamental building block for SAML federation into AWS is configuring the IdP in IAM and using IAM roles. SSO builds upon this. Given the options, B is a more foundational and direct description of the core mechanism."
    },
    {
        "id": 58,
        "domain": "Design Resilient Architectures - Scenario",
        "questionText": "A company is running a critical application on EC2 instances. They need to ensure that if an EC2 instance becomes unresponsive, it is automatically terminated and replaced by a new, healthy instance. Which AWS services should be used together to achieve this?",
        "options": [
            {
                "letter": "A",
                "text": "Amazon CloudWatch Alarms and AWS Lambda."
            },
            {
                "letter": "B",
                "text": "Elastic Load Balancing health checks and an Auto Scaling group with health check replacements."
            },
            {
                "letter": "C",
                "text": "AWS Systems Manager and manual instance reboot."
            },
            {
                "letter": "D",
                "text": "Amazon Route 53 health checks and DNS failover."
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **Elastic Load Balancing health checks and an Auto Scaling group with health check replacements:** An Auto Scaling group can be configured to use ELB health checks (or EC2 status checks). If an instance fails these health checks, the Auto Scaling group can automatically terminate the unhealthy instance and launch a new one to replace it, maintaining the desired capacity and application availability.\n\n**Why other options are incorrect:**\n\n*   **A) Amazon CloudWatch Alarms and AWS Lambda:** While you could build a custom solution with CloudWatch alarms triggering Lambda functions to replace instances, Auto Scaling groups provide this functionality natively and more robustly.\n*   **C) AWS Systems Manager and manual instance reboot:** This is not an automated replacement solution.\n*   **D) Amazon Route 53 health checks and DNS failover:** Route 53 can redirect traffic away from unhealthy endpoints, but it doesn't replace the failed EC2 instances themselves."
    },
    {
        "id": 59,
        "domain": "Design High-Performing Architectures - Scenario",
        "questionText": "A company has a web application that serves personalized content to users. The content generation is compute-intensive. They want to improve response times by caching the generated personalized content for a short period. Which caching strategy and AWS service would be most effective?",
        "options": [
            {
                "letter": "A",
                "text": "Client-side browser caching."
            },
            {
                "letter": "B",
                "text": "Caching at the CDN layer using Amazon CloudFront with a short TTL."
            },
            {
                "letter": "C",
                "text": "Using Amazon S3 as a cache."
            },
            {
                "letter": "D",
                "text": "Implementing an in-memory cache like Amazon ElastiCache (Redis or Memcached) at the application tier."
            }
        ],
        "correctAnswerLetter": "D",
        "explanation": "*   **Implementing an in-memory cache like Amazon ElastiCache (Redis or Memcached) at the application tier:** For personalized content that is compute-intensive to generate, caching it at the application tier using an in-memory cache like ElastiCache can significantly reduce latency. The application can check the cache first; if the content is present, it's served quickly. If not, it's generated, served, and then stored in the cache for subsequent requests.\n\n**Why other options are incorrect:**\n\n*   **A) Client-side browser caching:** While useful for static assets or less frequently changing public content, it's not ideal for personalized content that might change or needs to be centrally managed/invalidated.\n*   **B) Caching at the CDN layer using Amazon CloudFront with a short TTL:** CloudFront is excellent for caching public, static, or even some dynamic content. However, highly personalized content might result in a very low cache-hit ratio at the CDN if every user's content is unique, making an application-tier cache more effective.\n*   **C) Using Amazon S3 as a cache:** S3 is object storage and not designed as a low-latency in-memory cache for frequently accessed, personalized application data."
    },
    {
        "id": 60,
        "domain": "Design Cost-Optimized Architectures - Scenario",
        "questionText": "A company wants to analyze its AWS spending patterns and identify specific services or resources that are contributing most to its monthly bill. They need a tool that provides detailed breakdowns of costs by service, linked account, region, and tags. Which AWS tool is best for this detailed cost analysis?",
        "options": [
            {
                "letter": "A",
                "text": "AWS Budgets reports"
            },
            {
                "letter": "B",
                "text": "AWS Cost and Usage Report (CUR)"
            },
            {
                "letter": "C",
                "text": "AWS Trusted Advisor cost optimization checks"
            },
            {
                "letter": "D",
                "text": "Amazon CloudWatch billing metrics"
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **AWS Cost and Usage Report (CUR):** The CUR contains the most comprehensive set of AWS cost and usage data available, including additional metadata about AWS services, pricing, and reservations. It lists AWS usage for each service category used by an account and its IAM users in hourly or daily line items, as well as any tags that you have activated for cost allocation purposes. This report can be delivered to an S3 bucket and queried using tools like Amazon Athena or Amazon QuickSight for detailed analysis.\n\n**Why other options are incorrect:**\n\n*   **A) AWS Budgets reports:** Budgets help track spending against targets but don't provide the same level of granular detail as the CUR for deep analysis.\n*   **C) AWS Trusted Advisor cost optimization checks:** Trusted Advisor provides recommendations but not the raw, detailed cost and usage data for custom analysis.\n*   **D) Amazon CloudWatch billing metrics:** CloudWatch can show estimated charges but lacks the comprehensive breakdown available in the CUR."
    },
    {
        "id": 61,
        "domain": "Design Secure Architectures - Scenario",
        "questionText": "A company is deploying an application that requires access to an Amazon S3 bucket. To follow the principle of least privilege, how should the EC2 instances running the application be granted access to the S3 bucket?",
        "options": [
            {
                "letter": "A",
                "text": "Store AWS access keys directly on the EC2 instances."
            },
            {
                "letter": "B",
                "text": "Create an IAM user with S3 access and share the credentials with the application."
            },
            {
                "letter": "C",
                "text": "Assign an IAM role with the necessary S3 permissions to the EC2 instances."
            },
            {
                "letter": "D",
                "text": "Allow public read access to the S3 bucket."
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **Assign an IAM role with the necessary S3 permissions to the EC2 instances:** This is the most secure and recommended method. IAM roles provide temporary security credentials to the EC2 instances, eliminating the need to store long-term access keys on the instances. The permissions are defined in the IAM role and can be strictly scoped to only what the application needs.\n\n**Why other options are incorrect:**\n\n*   **A) Store AWS access keys directly on the EC2 instances:** This is a security risk. If the instance is compromised, the access keys can be stolen.\n*   **B) Create an IAM user with S3 access and share the credentials with the application:** This also involves managing long-term credentials and is less secure than using IAM roles for EC2 instances.\n*   **D) Allow public read access to the S3 bucket:** This should only be done if the data is truly intended for public consumption and is generally not a secure way to grant application access to private data."
    },
    {
        "id": 62,
        "domain": "Design Resilient Architectures - Scenario",
        "questionText": "A company wants to ensure its Amazon RDS database can be recovered to any point in time within the last 7 days with a granularity of 5 minutes. Which RDS features must be enabled and configured?",
        "options": [
            {
                "letter": "A",
                "text": "Multi-AZ deployment only."
            },
            {
                "letter": "B",
                "text": "Automated backups enabled with a backup retention period of at least 7 days."
            },
            {
                "letter": "C",
                "text": "Read Replicas in a different region."
            },
            {
                "letter": "D",
                "text": "Manual snapshots taken every 5 minutes."
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **Automated backups enabled with a backup retention period of at least 7 days:** Amazon RDS automated backups, along with transaction logs, allow for point-in-time recovery (PITR). You can restore your database to any specific second during your backup retention period (up to the last five minutes). Setting a retention period of at least 7 days meets the requirement.\n\n**Why other options are incorrect:**\n\n*   **A) Multi-AZ deployment only:** Multi-AZ is for high availability (failover), not for point-in-time recovery over several days.\n*   **C) Read Replicas in a different region:** Read Replicas are for read scaling and DR, not fine-grained PITR.\n*   **D) Manual snapshots taken every 5 minutes:** While manual snapshots can be taken, automated backups with transaction logs are the standard and more efficient way to achieve PITR. Taking manual snapshots every 5 minutes would be operationally burdensome and potentially costly."
    },
    {
        "id": 63,
        "domain": "Design High-Performing Architectures - Scenario",
        "questionText": "A company is experiencing high read latency on its Amazon DynamoDB table. The table has a well-distributed primary key, but read traffic is very high. Which DynamoDB feature can be used to offload read traffic and improve read performance without impacting write capacity?",
        "options": [
            {
                "letter": "A",
                "text": "DynamoDB Streams"
            },
            {
                "letter": "B",
                "text": "DynamoDB Global Tables"
            },
            {
                "letter": "C",
                "text": "DynamoDB Accelerator (DAX)"
            },
            {
                "letter": "D",
                "text": "Provisioned Throughput increase on the base table."
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **DynamoDB Accelerator (DAX):** DAX is a fully managed, highly available, in-memory cache for Amazon DynamoDB. DAX delivers up to a 10x performance improvement \u2013 from milliseconds to microseconds \u2013 even at millions of requests per second. It offloads read traffic from the underlying DynamoDB table, reducing read latency and cost.\n\n**Why other options are incorrect:**\n\n*   **A) DynamoDB Streams:** Streams capture item-level modifications in a DynamoDB table and are used for triggering other processes, not for caching reads.\n*   **B) DynamoDB Global Tables:** Global Tables provide multi-region, multi-active replication for DynamoDB, useful for globally distributed applications, but DAX is specifically for in-memory read caching.\n*   **D) Provisioned Throughput increase on the base table:** While increasing provisioned read capacity units (RCUs) on the table can handle more reads, DAX provides a more significant performance boost for read-heavy workloads by serving requests from an in-memory cache, often at a lower overall cost for those reads."
    },
    {
        "id": 64,
        "domain": "Design Cost-Optimized Architectures - Scenario",
        "questionText": "A company uses AWS Organizations to manage multiple AWS accounts. They want to benefit from volume discounts for services like S3 and EC2 across all their accounts. How can they achieve this?",
        "options": [
            {
                "letter": "A",
                "text": "By enabling AWS Cost Explorer in each member account."
            },
            {
                "letter": "B",
                "text": "By enabling consolidated billing in AWS Organizations."
            },
            {
                "letter": "C",
                "text": "By purchasing Reserved Instances separately in each member account."
            },
            {
                "letter": "D",
                "text": "By using AWS Budgets to allocate costs to member accounts."
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **By enabling consolidated billing in AWS Organizations:** Consolidated billing is a feature of AWS Organizations that allows you to consolidate payment for multiple AWS accounts. With consolidated billing, you receive a single bill for all accounts in your organization. A key benefit is that AWS combines the usage from all accounts to determine which volume pricing tiers to apply, potentially resulting in a lower overall bill as your aggregated usage qualifies for higher discount tiers.\n\n**Why other options are incorrect:**\n\n*   **A) By enabling AWS Cost Explorer in each member account:** Cost Explorer helps analyze costs but doesn't enable volume discounts across accounts.\n*   **C) By purchasing Reserved Instances separately in each member account:** While RIs provide discounts, consolidated billing allows RI sharing (if enabled) and volume discounts to apply more broadly.\n*   **D) By using AWS Budgets to allocate costs to member accounts:** Budgets are for tracking and alerting on costs, not for enabling discounts."
    },
    {
        "id": 65,
        "domain": "Design Secure Architectures - Scenario",
        "questionText": "A company needs to monitor the configuration of its AWS resources and ensure they comply with internal policies and regulatory standards. For example, they want to be alerted if an S3 bucket is made public or if security groups allow unrestricted inbound access. Which AWS service is designed for this type of configuration auditing and compliance checking?",
        "options": [
            {
                "letter": "A",
                "text": "Amazon GuardDuty"
            },
            {
                "letter": "B",
                "text": "AWS Security Hub"
            },
            {
                "letter": "C",
                "text": "AWS Config"
            },
            {
                "letter": "D",
                "text": "Amazon Inspector"
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **AWS Config:** This service continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations. You can use AWS Config rules (predefined or custom) to check for compliance with policies and standards, and get alerted to non-compliant resources.\n\n**Why other options are incorrect:**\n\n*   **A) Amazon GuardDuty:** GuardDuty is a threat detection service that monitors for malicious activity.\n*   **B) AWS Security Hub:** Security Hub provides a comprehensive view of your security posture by aggregating findings from various services, including AWS Config. However, AWS Config is the service that performs the actual configuration recording and compliance checking.\n*   **D) Amazon Inspector:** Inspector assesses applications for vulnerabilities."
    },
    {
        "id": 66,
        "domain": "Design Resilient Architectures - Scenario",
        "questionText": "A company has a business-critical application with a database running on Amazon RDS. They need to ensure that in the event of a regional outage, they can recover the database and application in another AWS Region within 1 hour (RTO) and with no more than 15 minutes of data loss (RPO). Which DR strategy for RDS would best meet these requirements?",
        "options": [
            {
                "letter": "A",
                "text": "Using RDS Multi-AZ deployment within the primary region only."
            },
            {
                "letter": "B",
                "text": "Taking daily snapshots and copying them to another region, then restoring manually."
            },
            {
                "letter": "C",
                "text": "Configuring cross-Region automated backups or using cross-Region Read Replicas that can be promoted."
            },
            {
                "letter": "D",
                "text": "Relying on S3 replication for database files."
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **Configuring cross-Region automated backups or using cross-Region Read Replicas that can be promoted:** Amazon RDS allows you to copy automated or manual snapshots to another region. You can also create a cross-Region Read Replica. For DR, you can restore from the copied snapshot or promote the Read Replica to a standalone instance in the DR region. Promoting a Read Replica is generally faster and can offer a lower RPO (seconds to minutes of replication lag) than restoring from a snapshot. Cross-region automated backups also allow point-in-time restore in the DR region. Both can meet the RTO/RPO if configured appropriately.\n\n**Why other options are incorrect:**\n\n*   **A) Using RDS Multi-AZ deployment within the primary region only:** Multi-AZ protects against AZ failure, not regional outages.\n*   **B) Taking daily snapshots and copying them to another region, then restoring manually:** Daily snapshots mean an RPO of up to 24 hours, which doesn't meet the 15-minute RPO. Manual restore can also be slow.\n*   **D) Relying on S3 replication for database files:** This is not a standard or recommended way to achieve DR for RDS databases."
    },
    {
        "id": 67,
        "domain": "Design High-Performing Architectures - Scenario",
        "questionText": "A company is building a microservices-based application. They need a way for services to communicate asynchronously without being tightly coupled. Messages between services can be up to 256 KB and need to be processed in the order they are sent for specific workflows. Which AWS messaging service is most appropriate?",
        "options": [
            {
                "letter": "A",
                "text": "Amazon SNS (Simple Notification Service)"
            },
            {
                "letter": "B",
                "text": "Amazon SQS (Simple Queue Service) Standard Queues"
            },
            {
                "letter": "C",
                "text": "Amazon SQS (Simple Queue Service) FIFO Queues"
            },
            {
                "letter": "D",
                "text": "Amazon Kinesis Data Streams"
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **Amazon SQS (Simple Queue Service) FIFO Queues:** FIFO (First-In, First-Out) queues are designed to enhance messaging between applications when the order of operations and events is critical, or where duplicates cannot be tolerated. They provide exactly-once processing and preserve the order in which messages are sent and received. They support messages up to 256 KB.\n\n**Why other options are incorrect:**\n\n*   **A) Amazon SNS (Simple Notification Service):** SNS is a pub/sub messaging service. While it can be used for asynchronous communication, it doesn't guarantee message ordering for subscribers in the same way SQS FIFO queues do.\n*   **B) Amazon SQS (Simple Queue Service) Standard Queues:** Standard queues offer at-least-once delivery and best-effort ordering. They do not guarantee message order, which is a requirement.\n*   **D) Amazon Kinesis Data Streams:** Kinesis is for real-time streaming data. While it maintains order within a shard, SQS FIFO queues are more specifically designed for ordered, exactly-once message processing between application components."
    },
    {
        "id": 68,
        "domain": "Design Cost-Optimized Architectures - Scenario",
        "questionText": "A company has a large S3 bucket with billions of small objects. They are incurring significant costs due to the high number of S3 GET requests. Most of these requests are for a small subset of frequently accessed objects. How can they reduce the S3 GET request costs while improving performance for these frequently accessed objects?",
        "options": [
            {
                "letter": "A",
                "text": "Use S3 Intelligent-Tiering."
            },
            {
                "letter": "B",
                "text": "Implement Amazon CloudFront as a cache in front of the S3 bucket."
            },
            {
                "letter": "C",
                "text": "Enable S3 Transfer Acceleration."
            },
            {
                "letter": "D",
                "text": "Switch to S3 One Zone-Infrequent Access for the entire bucket."
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **Implement Amazon CloudFront as a cache in front of the S3 bucket:** CloudFront can cache frequently accessed S3 objects at edge locations closer to users. Subsequent requests for these cached objects are served from the CloudFront cache, reducing the number of GET requests made directly to S3, thereby lowering S3 costs and improving performance for users.\n\n**Why other options are incorrect:**\n\n*   **A) Use S3 Intelligent-Tiering:** This optimizes storage costs by moving objects between access tiers but doesn't directly reduce GET request costs by caching.\n*   **C) Enable S3 Transfer Acceleration:** This speeds up uploads/downloads to S3 over long distances but doesn't reduce the number of GET requests or cache objects.\n*   **D) Switch to S3 One Zone-Infrequent Access for the entire bucket:** This would reduce storage costs for infrequently accessed data but would increase retrieval costs and doesn't address the high GET request volume for frequently accessed objects."
    },
    {
        "id": 69,
        "domain": "Design Secure Architectures - Scenario",
        "questionText": "A company needs to provide its developers with command-line access to EC2 instances in private subnets for troubleshooting and maintenance. They want to avoid using bastion hosts and opening SSH ports directly from the internet. Which AWS service allows secure shell access to instances without requiring public IP addresses or bastion hosts?",
        "options": [
            {
                "letter": "A",
                "text": "AWS Systems Manager Session Manager"
            },
            {
                "letter": "B",
                "text": "Amazon VPC Endpoints for EC2"
            },
            {
                "letter": "C",
                "text": "AWS Client VPN"
            },
            {
                "letter": "D",
                "text": "EC2 Instance Connect"
            }
        ],
        "correctAnswerLetter": "A",
        "explanation": "*   **AWS Systems Manager Session Manager:** Session Manager provides secure and auditable instance management without the need to open inbound SSH ports, manage bastion hosts, or manage SSH keys. It allows you to connect to your instances via a browser-based shell or through the AWS CLI. Instances only need the Systems Manager Agent installed and appropriate IAM permissions.\n\n**Why other options are incorrect:**\n\n*   **B) Amazon VPC Endpoints for EC2:** These allow private access to EC2 APIs, not shell access to instances.\n*   **C) AWS Client VPN:** This provides VPN access into a VPC, which could then allow SSH, but Session Manager is a more direct and often simpler solution for secure shell access without bastion hosts.\n*   **D) EC2 Instance Connect:** This provides a simple and secure way to connect to your instances using SSH. While it can use IAM for authorization, it typically requires the instance to have a public IP or be reachable via a bastion/VPN if it's in a private subnet. Session Manager avoids these requirements more effectively for private instances."
    },
    {
        "id": 70,
        "domain": "Design Resilient Architectures - Scenario",
        "questionText": "A company is using an Application Load Balancer (ALB) to distribute traffic to a fleet of EC2 instances. They want to ensure that if the application on an instance becomes unhealthy (e.g., returns HTTP 5xx errors), the ALB stops sending traffic to that instance. How should this be configured?",
        "options": [
            {
                "letter": "A",
                "text": "By configuring NACLs to block traffic to unhealthy instances."
            },
            {
                "letter": "B",
                "text": "By configuring Security Groups to deny traffic from the ALB to unhealthy instances."
            },
            {
                "letter": "C",
                "text": "By configuring health checks on the ALB target group and setting appropriate thresholds."
            },
            {
                "letter": "D",
                "text": "By using Amazon Route 53 health checks to remove unhealthy instances from DNS."
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **By configuring health checks on the ALB target group and setting appropriate thresholds:** ALBs perform health checks on registered targets (EC2 instances, Lambda functions, etc.) in a target group. If a target fails a specified number of consecutive health checks, the ALB marks it as unhealthy and stops routing traffic to it until it passes health checks again. You can configure the health check protocol, port, path, and thresholds.\n\n**Why other options are incorrect:**\n\n*   **A) By configuring NACLs to block traffic to unhealthy instances:** NACLs operate at the subnet level and are not suitable for dynamic health-based traffic routing by an ALB.\n*   **B) By configuring Security Groups to deny traffic from the ALB to unhealthy instances:** Security groups are stateful firewalls at the instance level. While they control traffic, they are not the mechanism ALBs use for health-based routing decisions.\n*   **D) By using Amazon Route 53 health checks to remove unhealthy instances from DNS:** Route 53 health checks are for DNS-level failover, not for an ALB managing traffic to backend instances within a target group."
    },
    {
        "id": 71,
        "domain": "Design High-Performing Architectures - Scenario",
        "questionText": "A company has a large number of IoT devices sending telemetry data to AWS. They need a scalable and managed service to ingest this data, filter it, transform it, and then route it to multiple downstream services like Amazon S3, Amazon Kinesis Data Streams, and AWS Lambda. Which AWS service is designed for this IoT data ingestion and processing pipeline?",
        "options": [
            {
                "letter": "A",
                "text": "Amazon MQ"
            },
            {
                "letter": "B",
                "text": "AWS IoT Core"
            },
            {
                "letter": "C",
                "text": "Amazon API Gateway"
            },
            {
                "letter": "D",
                "text": "AWS Step Functions"
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **AWS IoT Core:** This is a managed cloud service that lets connected devices easily and securely interact with cloud applications and other devices. AWS IoT Core can support billions of devices and trillions of messages, and can process and route those messages to AWS endpoints and to other devices reliably and securely. It includes features like a rules engine for filtering, transforming, and routing data.\n\n**Why other options are incorrect:**\n\n*   **A) Amazon MQ:** This is a managed message broker service for Apache ActiveMQ and RabbitMQ, not specifically designed for IoT device ingestion and management at scale in the same way IoT Core is.\n*   **C) Amazon API Gateway:** API Gateway is for creating, publishing, and managing APIs. While IoT devices could send data to an API, IoT Core provides a more comprehensive suite of features for device management, security, and data processing specific to IoT use cases.\n*   **D) AWS Step Functions:** Step Functions is a serverless function orchestrator for coordinating components of distributed applications. It could be a downstream service but is not the primary ingestion and routing service for IoT data."
    },
    {
        "id": 72,
        "domain": "Design Cost-Optimized Architectures - Scenario",
        "questionText": "A company is using Amazon EC2 instances for a workload that requires a consistent baseline level of compute capacity but also experiences occasional, unpredictable peaks in demand. They want to optimize costs for the baseline capacity while having the ability to scale for peaks. Which combination of EC2 purchasing options is most cost-effective?",
        "options": [
            {
                "letter": "A",
                "text": "Using only Spot Instances for both baseline and peak."
            },
            {
                "letter": "B",
                "text": "Using only On-Demand Instances for both baseline and peak."
            },
            {
                "letter": "C",
                "text": "Using Reserved Instances or Savings Plans for the baseline capacity, and On-Demand or Spot Instances for peak capacity."
            },
            {
                "letter": "D",
                "text": "Using only Dedicated Hosts."
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **Using Reserved Instances or Savings Plans for the baseline capacity, and On-Demand or Spot Instances for peak capacity:** This is a common cost optimization strategy. Reserved Instances (RIs) or Savings Plans provide significant discounts for the predictable, baseline compute capacity. For unpredictable peak demand, On-Demand instances provide flexibility, or Spot Instances can be used if the peak workload is fault-tolerant, offering even greater savings.\n\n**Why other options are incorrect:**\n\n*   **A) Using only Spot Instances for both baseline and peak:** While cost-effective, Spot Instances can be interrupted, making them unsuitable for baseline capacity that needs to be consistently available.\n*   **B) Using only On-Demand Instances for both baseline and peak:** This is flexible but doesn't provide any discounts for the continuous baseline usage.\n*   **D) Using only Dedicated Hosts:** These are physical servers dedicated to your use and are typically more expensive, used for specific compliance or licensing needs, not general cost optimization for variable workloads."
    },
    {
        "id": 73,
        "domain": "Design Secure Architectures - Scenario",
        "questionText": "A company needs to encrypt data at rest for its Amazon EBS volumes attached to EC2 instances. They want to use AWS-managed encryption keys and ensure minimal operational overhead. Which EBS encryption option should they choose?",
        "options": [
            {
                "letter": "A",
                "text": "Client-side encryption before writing data to EBS."
            },
            {
                "letter": "B",
                "text": "EBS encryption using AWS Key Management Service (KMS) with a customer master key (CMK) that they manage."
            },
            {
                "letter": "C",
                "text": "EBS encryption using the default AWS-managed KMS key for EBS."
            },
            {
                "letter": "D",
                "text": "Third-party disk encryption software running on the EC2 instances."
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **EBS encryption using the default AWS-managed KMS key for EBS:** Amazon EBS encryption offers a straightforward way to encrypt your EBS volumes. When you create an encrypted EBS volume, AWS handles the key management using AWS KMS. You can use the default AWS-managed CMK for EBS, which requires no key creation or management effort on your part, or you can specify a customer-managed CMK. For minimal operational overhead with AWS-managed keys, the default key is the simplest.\n\n**Why other options are incorrect:**\n\n*   **A) Client-side encryption before writing data to EBS:** This adds complexity to the application and is not server-side EBS encryption.\n*   **B) EBS encryption using AWS Key Management Service (KMS) with a customer master key (CMK) that they manage:** While this provides more control over the key, it involves slightly more operational overhead (creating and managing the CMK) than using the default AWS-managed key.\n*   **D) Third-party disk encryption software running on the EC2 instances:** This adds management overhead and complexity compared to native EBS encryption."
    },
    {
        "id": 74,
        "domain": "Design Resilient Architectures - Scenario",
        "questionText": "A company has a web application that uses Amazon S3 to store user-uploaded images. They need to ensure that these images are highly durable and protected against accidental deletion. Which S3 features should be enabled?",
        "options": [
            {
                "letter": "A",
                "text": "S3 Transfer Acceleration and S3 Requester Pays."
            },
            {
                "letter": "B",
                "text": "S3 Versioning and MFA Delete."
            },
            {
                "letter": "C",
                "text": "S3 Lifecycle Policies and S3 Intelligent-Tiering."
            },
            {
                "letter": "D",
                "text": "S3 Object Lock and S3 Glacier Deep Archive."
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **S3 Versioning and MFA Delete:** S3 Versioning keeps multiple versions of an object in the same bucket. This means that if an object is deleted or overwritten, you can easily retrieve previous versions. MFA (Multi-Factor Authentication) Delete adds another layer of security by requiring an MFA code to permanently delete an object version or change the versioning state of a bucket. Together, these provide strong protection against accidental deletions and ensure high durability.\n\n**Why other options are incorrect:**\n\n*   **A) S3 Transfer Acceleration and S3 Requester Pays:** These are for optimizing data transfer and cost allocation, not for data durability or protection against deletion.\n*   **C) S3 Lifecycle Policies and S3 Intelligent-Tiering:** These are for managing object lifecycles and optimizing storage costs, not primarily for protection against accidental deletion (though lifecycle policies can be configured to retain versions).\n*   **D) S3 Object Lock and S3 Glacier Deep Archive:** Object Lock provides WORM protection, which might be too restrictive if images need to be updated or legitimately deleted. Glacier Deep Archive is for long-term, low-cost archival, not necessarily the primary storage for actively used images needing protection from accidental deletion."
    },
    {
        "id": 75,
        "domain": "Design High-Performing Architectures - Scenario",
        "questionText": "A company is deploying a containerized application on Amazon ECS. They need a load balancing solution that can route traffic to individual containers based on path or host, and supports dynamic port mapping for containers. Which AWS load balancer type is most suitable?",
        "options": [
            {
                "letter": "A",
                "text": "Classic Load Balancer (CLB)"
            },
            {
                "letter": "B",
                "text": "Network Load Balancer (NLB)"
            },
            {
                "letter": "C",
                "text": "Application Load Balancer (ALB)"
            },
            {
                "letter": "D",
                "text": "Gateway Load Balancer (GWLB)"
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **Application Load Balancer (ALB):** ALBs operate at the application layer (Layer 7) and are ideal for HTTP/HTTPS traffic. They support path-based routing, host-based routing, and can route traffic to multiple target groups, making them well-suited for microservices and containerized applications. ALBs integrate seamlessly with Amazon ECS, supporting dynamic port mapping, which allows you to run multiple tasks from a single service on the same EC2 instance using different ports.\n\n**Why other options are incorrect:**\n\n*   **A) Classic Load Balancer (CLB):** CLBs are a previous generation load balancer. While they can work with ECS, ALBs offer more advanced features like path-based routing and better integration for container workloads.\n*   **B) Network Load Balancer (NLB):** NLBs operate at the transport layer (Layer 4) and are designed for ultra-high performance and low latency. They don't support path-based or host-based routing. While they can be used with ECS, ALBs are generally a better fit for typical HTTP/HTTPS containerized applications needing Layer 7 features.\n*   **D) Gateway Load Balancer (GWLB):** GWLBs are used to deploy, scale, and manage third-party virtual network appliances like firewalls and intrusion detection systems. They are not for general application load balancing."
    },
    {
        "id": 76,
        "domain": "Design Secure Architectures - Scenario",
        "questionText": "A company wants to implement a centralized logging solution for security and operational auditing of their AWS resources. They need to collect logs from various services like CloudTrail, VPC Flow Logs, and application logs from EC2 instances. Which AWS service is most suitable for aggregating and analyzing these logs?",
        "options": [
            {
                "letter": "A",
                "text": "Amazon CloudWatch Logs"
            },
            {
                "letter": "B",
                "text": "AWS X-Ray"
            },
            {
                "letter": "C",
                "text": "Amazon S3 with Athena"
            },
            {
                "letter": "D",
                "text": "AWS Security Hub"
            }
        ],
        "correctAnswerLetter": "A",
        "explanation": "*   **Amazon CloudWatch Logs:** CloudWatch Logs allows you to monitor, store, and access your log files from Amazon Elastic Compute Cloud (Amazon EC2) instances, AWS CloudTrail, Route 53, and other sources. You can then retrieve the associated log data from CloudWatch Logs. It's a centralized service for log aggregation and can be used with CloudWatch Logs Insights for analysis.\n\n**Why other options are incorrect:**\n\n*   **B) AWS X-Ray:** X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. It's for tracing application requests, not general log aggregation.\n*   **C) Amazon S3 with Athena:** While logs can be stored in S3 and queried with Athena, CloudWatch Logs provides a more integrated and real-time solution specifically designed for log aggregation and monitoring from various AWS services.\n*   **D) AWS Security Hub:** Security Hub provides a comprehensive view of your security alerts and security posture across your AWS accounts. It consumes logs and findings from other services but is not the primary log aggregation service itself."
    },
    {
        "id": 77,
        "domain": "Design Resilient Architectures - Scenario",
        "questionText": "A company is designing a new application that will be deployed across multiple AWS Regions for global reach and disaster recovery. They need a managed DNS service that can route users to the nearest healthy region and provide failover capabilities. Which AWS service should they use?",
        "options": [
            {
                "letter": "A",
                "text": "Amazon Route 53"
            },
            {
                "letter": "B",
                "text": "Elastic Load Balancing (ELB)"
            },
            {
                "letter": "C",
                "text": "AWS Direct Connect"
            },
            {
                "letter": "D",
                "text": "Amazon CloudFront"
            }
        ],
        "correctAnswerLetter": "A",
        "explanation": "*   **Amazon Route 53:** Route 53 is a scalable and highly available Domain Name System (DNS) web service. It can be used to route users to internet applications by translating names like www.example.com into the numeric IP addresses like 192.0.2.1 that computers use to connect to each other. It supports various routing policies, including latency-based routing (routes users to the region with the lowest latency) and failover routing (routes traffic to a healthy resource if the primary resource is unhealthy). This makes it ideal for managing traffic for multi-region applications.\n\n**Why other options are incorrect:**\n\n*   **B) Elastic Load Balancing (ELB):** ELB operates within a region to distribute traffic to backend instances. It doesn't provide global DNS routing or inter-region failover.\n*   **C) AWS Direct Connect:** Direct Connect provides a dedicated network connection from on-premises to AWS. It's not a DNS or global traffic management service.\n*   **D) Amazon CloudFront:** CloudFront is a CDN that caches content at edge locations. While it improves performance and can route to origins, Route 53 is the foundational service for DNS-based global traffic management and failover."
    },
    {
        "id": 78,
        "domain": "Design High-Performing Architectures - Scenario",
        "questionText": "A company needs to process large volumes of streaming data from clickstream logs and social media feeds in real-time. The processed data will be used for dashboards and real-time analytics. Which AWS service is best suited for building this real-time data processing pipeline?",
        "options": [
            {
                "letter": "A",
                "text": "Amazon SQS with EC2 workers"
            },
            {
                "letter": "B",
                "text": "Amazon Kinesis (Data Streams, Data Firehose, Data Analytics)"
            },
            {
                "letter": "C",
                "text": "AWS Batch"
            },
            {
                "letter": "D",
                "text": "AWS Glue"
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **Amazon Kinesis:** The Amazon Kinesis family of services is designed for real-time streaming data. \n    *   **Kinesis Data Streams** can collect and process large streams of data records in real time. \n    *   **Kinesis Data Firehose** can load streaming data into data stores and analytics tools. \n    *   **Kinesis Data Analytics** can process and analyze streaming data using SQL or Apache Flink. This suite of services is ideal for building real-time analytics and processing pipelines.\n\n**Why other options are incorrect:**\n\n*   **A) Amazon SQS with EC2 workers:** SQS is a message queue, suitable for decoupling. While it can handle streaming data, Kinesis is purpose-built for high-throughput, real-time stream processing and analytics.\n*   **C) AWS Batch:** AWS Batch is for batch computing workloads, not real-time stream processing.\n*   **D) AWS Glue:** AWS Glue is a fully managed extract, transform, and load (ETL) service. While it can process data, it's more geared towards batch ETL jobs rather than continuous real-time stream processing."
    },
    {
        "id": 79,
        "domain": "Design Cost-Optimized Architectures - Scenario",
        "questionText": "A company has a legacy monolithic application running on a large, expensive EC2 instance. They want to modernize the application and reduce costs by breaking it down into smaller, independent microservices that can scale independently. Which AWS compute service would be most cost-effective for deploying these microservices, assuming they can be containerized?",
        "options": [
            {
                "letter": "A",
                "text": "Amazon EC2 Spot Instances"
            },
            {
                "letter": "B",
                "text": "AWS Lambda"
            },
            {
                "letter": "C",
                "text": "Amazon Lightsail"
            },
            {
                "letter": "D",
                "text": "AWS Fargate or Amazon EKS with EC2 (if Spot/Savings Plans are used)"
            }
        ],
        "correctAnswerLetter": "D",
        "explanation": "*   **AWS Fargate or Amazon EKS with EC2:** For containerized microservices, AWS Fargate (serverless compute for containers) or Amazon EKS (managed Kubernetes service) provide robust platforms. Fargate abstracts away the underlying EC2 instances, and you pay for vCPU and memory resources consumed. EKS allows more control over the EC2 instances. Both can be cost-effective for microservices, especially if Spot Instances or Savings Plans are utilized with EKS on EC2, as they allow for scaling individual services based on demand rather than scaling a large monolith.\n\n**Why other options are incorrect:**\n\n*   **A) Amazon EC2 Spot Instances:** While cost-effective, directly managing a fleet of Spot Instances for numerous microservices can be complex without an orchestrator like EKS or Fargate.\n*   **B) AWS Lambda:** Lambda is excellent for event-driven, serverless functions. While microservices can be built with Lambda, if the existing application is being containerized, Fargate/EKS is a more direct path for deploying those containers.\n*   **C) Amazon Lightsail:** Lightsail provides simplified virtual private servers and is not typically used for complex microservice architectures requiring dynamic scaling and orchestration."
    },
    {
        "id": 80,
        "domain": "Design Secure Architectures - Scenario",
        "questionText": "A company is concerned about Distributed Denial of Service (DDoS) attacks targeting its public-facing web applications hosted on AWS. Which AWS service provides protection against common and sophisticated DDoS attacks?",
        "options": [
            {
                "letter": "A",
                "text": "AWS WAF (Web Application Firewall)"
            },
            {
                "letter": "B",
                "text": "Amazon GuardDuty"
            },
            {
                "letter": "C",
                "text": "AWS Shield (Standard and Advanced)"
            },
            {
                "letter": "D",
                "text": "Security Groups"
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **AWS Shield (Standard and Advanced):** AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS. AWS Shield Standard is automatically enabled for all AWS customers at no additional cost and defends against most common, frequently occurring network and transport layer DDoS attacks. For higher levels of protection against larger and more sophisticated attacks, AWS Shield Advanced provides expanded DDoS attack protection for your applications running on Amazon EC2, Elastic Load Balancing (ELB), Amazon CloudFront, AWS Global Accelerator, and Route 53.\n\n**Why other options are incorrect:**\n\n*   **A) AWS WAF:** WAF protects against application layer attacks like SQL injection or XSS, not primarily network/transport layer DDoS attacks.\n*   **B) Amazon GuardDuty:** GuardDuty is a threat detection service, it identifies malicious activity but doesn't directly mitigate DDoS attacks.\n*   **D) Security Groups:** Security groups act as a virtual firewall for instances to control inbound and outbound traffic at the protocol and port level. They do not provide specific DDoS mitigation capabilities."
    },
    {
        "id": 81,
        "domain": "Design Resilient Architectures - Scenario",
        "questionText": "A company has a critical application hosted on EC2 instances. They require a database solution that offers automatic failover across multiple Availability Zones (AZs) to ensure business continuity. Which configuration of Amazon RDS would best meet this requirement?",
        "options": [
            {
                "letter": "A",
                "text": "RDS deployed in a single AZ with frequent manual backups."
            },
            {
                "letter": "B",
                "text": "RDS deployed with Read Replicas in different AZs."
            },
            {
                "letter": "C",
                "text": "RDS deployed with Multi-AZ configuration."
            },
            {
                "letter": "D",
                "text": "RDS deployed on an EC2 instance with manual database replication to another instance in a different AZ."
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **RDS deployed with Multi-AZ configuration:** Amazon RDS Multi-AZ deployments enhance database availability and durability. When you provision a Multi-AZ DB instance, Amazon RDS automatically creates a primary DB instance and synchronously replicates the data to a standby instance in a different Availability Zone. In the event of an infrastructure failure, Amazon RDS performs an automatic failover to the standby instance, minimizing downtime.\n\n**Why other options are incorrect:**\n\n*   **A) RDS deployed in a single AZ with frequent manual backups:** This does not provide automatic failover and has a higher RTO/RPO.\n*   **B) RDS deployed with Read Replicas in different AZs:** Read Replicas are primarily for read scaling and are asynchronously replicated. While they can be promoted, it's not an automatic failover mechanism for the primary database in the same way Multi-AZ is.\n*   **D) RDS deployed on an EC2 instance with manual database replication:** This is a self-managed solution that lacks the automated failover, managed maintenance, and potentially the resilience of the RDS Multi-AZ service."
    },
    {
        "id": 82,
        "domain": "Design High-Performing Architectures - Scenario",
        "questionText": "A company is building a new web application that is expected to receive millions of requests per second from users across the globe. They need a solution that can cache frequently accessed static and dynamic content at edge locations to reduce latency and improve user experience. Which AWS service is most suitable for this?",
        "options": [
            {
                "letter": "A",
                "text": "Amazon S3 with Cross-Region Replication"
            },
            {
                "letter": "B",
                "text": "Elastic Load Balancing (Application Load Balancer)"
            },
            {
                "letter": "C",
                "text": "Amazon CloudFront"
            },
            {
                "letter": "D",
                "text": "AWS Global Accelerator"
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **Amazon CloudFront:** CloudFront is a content delivery network (CDN) that securely delivers data, videos, applications, and APIs to customers globally with low latency and high transfer speeds. It integrates with other AWS services and can cache static and dynamic content at edge locations closer to users, significantly improving performance for global applications.\n\n**Why other options are incorrect:**\n\n*   **A) Amazon S3 with Cross-Region Replication:** S3 is for storage, and while CRR replicates data, it doesn't act as a global caching layer for application content delivery.\n*   **B) Elastic Load Balancing (Application Load Balancer):** ALBs distribute traffic within a region, not globally for edge caching.\n*   **D) AWS Global Accelerator:** Global Accelerator improves application availability and performance by directing traffic to optimal endpoints over the AWS global network. While it can improve performance, CloudFront is specifically designed for content caching and delivery from edge locations."
    },
    {
        "id": 83,
        "domain": "Design Cost-Optimized Architectures - Scenario",
        "questionText": "A company has a large number of EC2 instances that are only used for a few hours each day for batch processing. They want to minimize costs. Which EC2 purchasing option would be the most cost-effective for this workload, assuming interruptions can be tolerated?",
        "options": [
            {
                "letter": "A",
                "text": "On-Demand Instances"
            },
            {
                "letter": "B",
                "text": "Reserved Instances"
            },
            {
                "letter": "C",
                "text": "Spot Instances"
            },
            {
                "letter": "D",
                "text": "Dedicated Hosts"
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **Spot Instances:** Spot Instances allow you to take advantage of unused EC2 capacity at significant discounts (up to 90% off On-Demand prices). For batch processing workloads that can tolerate interruptions (as Spot Instances can be reclaimed by AWS with a short notice), they offer the most cost-effective solution.\n\n**Why other options are incorrect:**\n\n*   **A) On-Demand Instances:** These are more expensive than Spot Instances for workloads that can be interrupted.\n*   **B) Reserved Instances:** RIs offer discounts for a commitment (1 or 3 years). If the usage is only for a few hours a day, the RI might not be fully utilized, making Spot Instances potentially more cost-effective for the actual usage period.\n*   **D) Dedicated Hosts:** These are physical servers dedicated for your use and are generally more expensive, suited for specific compliance or licensing needs."
    },
    {
        "id": 84,
        "domain": "Design Secure Architectures - Scenario",
        "questionText": "A company needs to ensure that all API calls made to their AWS resources are logged for security auditing and compliance purposes. They also need to be able to analyze these logs. Which AWS service is primarily designed for this?",
        "options": [
            {
                "letter": "A",
                "text": "Amazon CloudWatch"
            },
            {
                "letter": "B",
                "text": "AWS CloudTrail"
            },
            {
                "letter": "C",
                "text": "AWS Config"
            },
            {
                "letter": "D",
                "text": "AWS Security Hub"
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **AWS CloudTrail:** CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. It records API calls made on your account (including from the AWS Management Console, AWS SDKs, command line tools, and other AWS services) and delivers log files to an Amazon S3 bucket. These logs can then be analyzed.\n\n**Why other options are incorrect:**\n\n*   **A) Amazon CloudWatch:** CloudWatch monitors resources and applications, collecting metrics and logs. While it can store application logs, CloudTrail is specifically for AWS API call logging.\n*   **C) AWS Config:** Config records configuration changes to AWS resources and evaluates them against rules. It doesn't log all API calls.\n*   **D) AWS Security Hub:** Security Hub aggregates security findings from various services, including CloudTrail, but it's not the primary service for API call logging itself."
    },
    {
        "id": 85,
        "domain": "Design Resilient Architectures - Scenario",
        "questionText": "A company wants to deploy a web application with high availability across multiple Availability Zones. They also want to ensure that traffic is distributed evenly across their EC2 instances and that unhealthy instances are automatically removed from service. Which AWS services should they combine?",
        "options": [
            {
                "letter": "A",
                "text": "Amazon Route 53 with multiple EC2 instances."
            },
            {
                "letter": "B",
                "text": "Elastic Load Balancing (e.g., Application Load Balancer) with an Auto Scaling group across multiple AZs."
            },
            {
                "letter": "C",
                "text": "Amazon S3 for static content and EC2 for dynamic content, both in a single AZ."
            },
            {
                "letter": "D",
                "text": "AWS Lambda with API Gateway."
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **Elastic Load Balancing (e.g., Application Load Balancer) with an Auto Scaling group across multiple AZs:** This is a classic AWS architecture for high availability. The Auto Scaling group ensures that the desired number of instances are running across multiple AZs. The ELB distributes incoming traffic across these healthy instances and performs health checks, removing unhealthy instances from rotation. If an AZ fails, instances in other AZs can continue to serve traffic.\n\n**Why other options are incorrect:**\n\n*   **A) Amazon Route 53 with multiple EC2 instances:** Route 53 can provide DNS failover, but ELB and Auto Scaling are more directly involved in managing instance health and traffic distribution within a region.\n*   **C) Amazon S3 for static content and EC2 for dynamic content, both in a single AZ:** This lacks AZ-level fault tolerance.\n*   **D) AWS Lambda with API Gateway:** This is a serverless architecture. While highly available, the question implies an EC2-based deployment that needs load balancing and auto-scaling for resilience."
    },
    {
        "id": 86,
        "domain": "Design High-Performing Architectures - Scenario",
        "questionText": "A company needs to build a data lake on AWS to store and analyze petabytes of structured and semi-structured data. They require a storage service that is highly scalable, durable, and cost-effective, and can be easily integrated with analytics services like Amazon Athena and Amazon Redshift Spectrum. Which AWS service is most appropriate for the core storage of this data lake?",
        "options": [
            {
                "letter": "A",
                "text": "Amazon EBS"
            },
            {
                "letter": "B",
                "text": "Amazon Glacier"
            },
            {
                "letter": "C",
                "text": "Amazon S3"
            },
            {
                "letter": "D",
                "text": "Amazon DocumentDB"
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **Amazon S3 (Simple Storage Service):** S3 is an object storage service that offers industry-leading scalability, data availability, security, and performance. It is widely used as the foundation for data lakes due to its ability to store vast amounts of data in various formats, its cost-effectiveness, and its tight integration with AWS analytics services like Athena, Redshift Spectrum, EMR, and Glue.\n\n**Why other options are incorrect:**\n\n*   **A) Amazon EBS:** EBS provides block storage volumes for EC2 instances. It's not designed as a scalable, shared data lake storage solution.\n*   **B) Amazon Glacier:** Glacier (and Glacier Deep Archive) are low-cost archival storage services. While data might eventually be archived there, the primary, active data lake storage for analytics would typically be in more readily accessible S3 tiers.\n*   **D) Amazon DocumentDB:** This is a NoSQL document database service. While it can store semi-structured data, it's not typically used as the primary storage for a petabyte-scale data lake intended for broad analytics."
    },
    {
        "id": 87,
        "domain": "Design Cost-Optimized Architectures - Scenario",
        "questionText": "A company is running a stateless application on a large number of EC2 instances. The workload varies significantly throughout the day. To optimize costs, they want to automatically scale the number of instances up or down based on demand, ensuring they only pay for what they use. Which AWS feature or service is best suited for this?",
        "options": [
            {
                "letter": "A",
                "text": "EC2 Reserved Instances"
            },
            {
                "letter": "B",
                "text": "EC2 Auto Scaling"
            },
            {
                "letter": "C",
                "text": "AWS Budgets with spending limits"
            },
            {
                "letter": "D",
                "text": "AWS Cost Explorer recommendations"
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **EC2 Auto Scaling:** EC2 Auto Scaling helps you maintain application availability and allows you to automatically scale your Amazon EC2 capacity up or down according to conditions you define. You can use it to ensure that the number of EC2 instances you\u2019re using scales up seamlessly during demand spikes to maintain performance and decreases automatically during demand lulls to minimize costs. This is ideal for workloads with variable demand.\n\n**Why other options are incorrect:**\n\n*   **A) EC2 Reserved Instances:** RIs provide cost savings for a fixed capacity commitment, not for dynamic scaling based on demand.\n*   **C) AWS Budgets with spending limits:** Budgets help track costs but don't dynamically scale infrastructure.\n*   **D) AWS Cost Explorer recommendations:** Cost Explorer provides insights and recommendations but doesn't directly implement scaling actions."
    },
    {
        "id": 88,
        "domain": "Design Secure Architectures - Scenario",
        "questionText": "A company wants to ensure that all network traffic flowing into and out of its VPC is monitored and logged for security analysis and troubleshooting. They need to capture information about accepted and rejected traffic based on security group and NACL rules. Which AWS service can provide this detailed network traffic logging?",
        "options": [
            {
                "letter": "A",
                "text": "AWS CloudTrail logs"
            },
            {
                "letter": "B",
                "text": "VPC Flow Logs"
            },
            {
                "letter": "C",
                "text": "AWS WAF logs"
            },
            {
                "letter": "D",
                "text": "Elastic Load Balancer access logs"
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **VPC Flow Logs:** VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Flow log data can be published to Amazon CloudWatch Logs or Amazon S3. It helps with troubleshooting connectivity issues, monitoring network traffic, and identifying potential security threats by logging accepted and rejected traffic based on security group and NACL rules.\n\n**Why other options are incorrect:**\n\n*   **A) AWS CloudTrail logs:** CloudTrail logs API calls made to AWS services, not network traffic details within a VPC.\n*   **C) AWS WAF logs:** WAF logs web requests to applications protected by WAF, focusing on application-layer traffic, not all IP traffic within the VPC.\n*   **D) Elastic Load Balancer access logs:** These logs provide information about requests made to applications behind an ELB, not comprehensive network traffic logging for the entire VPC."
    },
    {
        "id": 89,
        "domain": "Design Resilient Architectures - Scenario",
        "questionText": "A company is designing a critical application that requires data to be replicated across at least three physically distinct locations within a region to ensure high durability and availability. Which AWS storage service inherently provides this level of data redundancy across multiple Availability Zones by default?",
        "options": [
            {
                "letter": "A",
                "text": "Amazon EBS (single volume)"
            },
            {
                "letter": "B",
                "text": "Amazon EC2 Instance Store"
            },
            {
                "letter": "C",
                "text": "Amazon S3 Standard storage class"
            },
            {
                "letter": "D",
                "text": "Amazon RDS Single-AZ deployment"
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **Amazon S3 Standard storage class:** Amazon S3 Standard offers high durability (99.999999999%) of objects by automatically storing data redundantly across multiple devices in multiple facilities (Availability Zones) within an AWS Region. This inherent redundancy protects against device failures and even entire AZ outages for data stored in S3.\n\n**Why other options are incorrect:**\n\n*   **A) Amazon EBS (single volume):** An EBS volume is tied to a specific AZ. While you can create snapshots and replicate them, a single volume itself is not inherently replicated across multiple AZs.\n*   **B) Amazon EC2 Instance Store:** This is temporary block-level storage directly attached to an EC2 instance. Data is lost if the instance is stopped or terminated. It does not offer multi-AZ redundancy.\n*   **D) Amazon RDS Single-AZ deployment:** As the name suggests, this deploys the database in a single AZ. For multi-AZ redundancy with RDS, you need to select the Multi-AZ deployment option."
    },
    {
        "id": 90,
        "domain": "Design High-Performing Architectures - Scenario",
        "questionText": "A company wants to deploy a global application that requires low latency access for users in different geographical locations. They need to serve static and dynamic content from locations closest to their users. Which combination of AWS services would be most effective?",
        "options": [
            {
                "letter": "A",
                "text": "Amazon S3 for static content and EC2 instances in a single region for dynamic content."
            },
            {
                "letter": "B",
                "text": "Amazon CloudFront with multiple origins (e.g., S3 for static, ELB/EC2 for dynamic) and edge locations worldwide."
            },
            {
                "letter": "C",
                "text": "AWS Global Accelerator pointing to application endpoints in a single region."
            },
            {
                "letter": "D",
                "text": "Using Amazon Route 53 Geolocation routing to direct users to region-specific EC2 instances."
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **Amazon CloudFront with multiple origins (e.g., S3 for static, ELB/EC2 for dynamic) and edge locations worldwide:** CloudFront is a global CDN that caches content at edge locations closer to users. It can serve both static content (e.g., from S3) and dynamic content (by forwarding requests to origin servers like ELB/EC2). This significantly reduces latency for users worldwide.\n\n**Why other options are incorrect:**\n\n*   **A) Amazon S3 for static content and EC2 instances in a single region for dynamic content:** This would result in high latency for users far from the single region hosting the dynamic content.\n*   **C) AWS Global Accelerator:** Global Accelerator improves availability and performance by directing traffic to optimal regional endpoints over the AWS global network. While it helps, CloudFront is more specifically designed for caching and delivering content from edge locations, which is key for low latency global applications.\n*   **D) Using Amazon Route 53 Geolocation routing:** Route 53 can direct users to different regional endpoints based on their geographic location, but CloudFront provides a more comprehensive solution with edge caching for both static and dynamic content, often leading to better performance."
    },
    {
        "id": 91,
        "domain": "Design Cost-Optimized Architectures - Scenario",
        "questionText": "A company is running a non-critical, fault-tolerant batch processing workload that can be interrupted. They want to use the most cost-effective EC2 instance purchasing option. Which option should they choose?",
        "options": [
            {
                "letter": "A",
                "text": "On-Demand Instances"
            },
            {
                "letter": "B",
                "text": "Reserved Instances"
            },
            {
                "letter": "C",
                "text": "Spot Instances"
            },
            {
                "letter": "D",
                "text": "Dedicated Hosts"
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **Spot Instances:** Spot Instances offer the largest discounts on EC2 compute, making them ideal for fault-tolerant workloads that can be interrupted, such as batch processing, data analysis, and background tasks. Since the workload can tolerate interruptions, Spot Instances provide the most significant cost savings.\n\n**Why other options are incorrect:**\n\n*   **A) On-Demand Instances:** These are more expensive than Spot Instances and are better suited for workloads that cannot tolerate interruptions.\n*   **B) Reserved Instances:** RIs offer discounts for a commitment but are generally more expensive than Spot Instances for interruptible workloads.\n*   **D) Dedicated Hosts:** These are physical servers dedicated for your use and are the most expensive option, typically used for specific compliance or licensing requirements."
    },
    {
        "id": 92,
        "domain": "Design Secure Architectures - Scenario",
        "questionText": "A company needs to ensure that all data transmitted between their on-premises network and their Amazon VPC is encrypted. They are using an AWS Site-to-Site VPN connection. Which protocol is typically used by AWS Site-to-Site VPN to provide this encryption?",
        "options": [
            {
                "letter": "A",
                "text": "SSL/TLS"
            },
            {
                "letter": "B",
                "text": "SSH"
            },
            {
                "letter": "C",
                "text": "IPsec"
            },
            {
                "letter": "D",
                "text": "HTTPS"
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **IPsec (Internet Protocol Security):** AWS Site-to-Site VPN uses IPsec tunnels to secure traffic between your on-premises network and your Amazon VPC. IPsec is a suite of protocols that provides security for Internet Protocol communications by authenticating and encrypting each IP packet of a communication session.\n\n**Why other options are incorrect:**\n\n*   **A) SSL/TLS:** SSL (Secure Sockets Layer) and TLS (Transport Layer Security) are cryptographic protocols designed to provide communications security over a computer network. They are commonly used for HTTPS, but IPsec is the standard for VPNs.\n*   **B) SSH (Secure Shell):** SSH is a cryptographic network protocol for operating network services securely over an unsecured network. It's typically used for secure command-line access, not for establishing VPN tunnels.\n*   **D) HTTPS:** HTTPS is HTTP over SSL/TLS, used for secure web communication. It operates at the application layer, whereas VPNs operate at lower network layers."
    },
    {
        "id": 93,
        "domain": "Design Resilient Architectures - Scenario",
        "questionText": "A company wants to design an application that can withstand the failure of an entire AWS Availability Zone without significant downtime. Which of the following is a key design principle to achieve this?",
        "options": [
            {
                "letter": "A",
                "text": "Deploying all application components to a single, powerful EC2 instance."
            },
            {
                "letter": "B",
                "text": "Distributing application components across multiple Availability Zones within a Region."
            },
            {
                "letter": "C",
                "text": "Regularly backing up data to Amazon S3 in the same Availability Zone."
            },
            {
                "letter": "D",
                "text": "Using only Spot Instances to reduce costs, accepting occasional downtime."
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **Distributing application components across multiple Availability Zones within a Region:** Availability Zones are isolated locations within an AWS Region. By designing applications to run in multiple AZs, you can protect them from the failure of a single location. If one AZ becomes unavailable, the application can continue to run from other AZs.\n\n**Why other options are incorrect:**\n\n*   **A) Deploying all application components to a single, powerful EC2 instance:** This creates a single point of failure. If that instance or its AZ fails, the application goes down.\n*   **C) Regularly backing up data to Amazon S3 in the same Availability Zone:** While backups are important, if the AZ itself fails, the S3 data within that AZ might also be affected or inaccessible for a period. Cross-AZ or cross-region replication is better for resilience.\n*   **D) Using only Spot Instances to reduce costs, accepting occasional downtime:** Spot Instances are not suitable for critical applications requiring high availability due to their interruptible nature."
    },
    {
        "id": 94,
        "domain": "Design High-Performing Architectures - Scenario",
        "questionText": "A company is building a global application that requires low-latency access to data for users around the world. The data is primarily read-only, but needs to be consistent globally. Which AWS database service and feature would be most suitable for this scenario?",
        "options": [
            {
                "letter": "A",
                "text": "Amazon RDS for MySQL with Multi-AZ deployment."
            },
            {
                "letter": "B",
                "text": "Amazon DynamoDB with On-Demand capacity."
            },
            {
                "letter": "C",
                "text": "Amazon Aurora Global Database."
            },
            {
                "letter": "D",
                "text": "Amazon ElastiCache for Redis with cluster mode enabled."
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **Amazon Aurora Global Database:** This is designed for globally distributed applications, allowing a single Aurora database to span multiple AWS Regions. It provides low-latency global reads by allowing applications to read from a nearby replica, and supports write forwarding to the primary region. Data replication across regions is typically under one second, ensuring relatively up-to-date information for global users.\n\n**Why other options are incorrect:**\n\n*   **A) Amazon RDS for MySQL with Multi-AZ deployment:** Multi-AZ is for high availability within a single region, not for global low-latency reads.\n*   **B) Amazon DynamoDB with On-Demand capacity:** While DynamoDB can be used globally, Aurora Global Database is specifically optimized for relational database workloads requiring global distribution and low-latency reads.\n*   **D) Amazon ElastiCache for Redis with cluster mode enabled:** ElastiCache is an in-memory cache. While it can improve read performance, it's not a primary database solution for persistent global data storage and consistency in the same way Aurora Global Database is designed."
    },
    {
        "id": 95,
        "domain": "Design Cost-Optimized Architectures - Scenario",
        "questionText": "A company has a legacy application running on an older generation EC2 instance type. They want to move to a newer, more cost-effective instance type without changing the application code. Which AWS tool or service can help them identify potential cost savings by rightsizing their EC2 instances?",
        "options": [
            {
                "letter": "A",
                "text": "AWS Cost Explorer (Rightsizing Recommendations)"
            },
            {
                "letter": "B",
                "text": "AWS Trusted Advisor (Cost Optimization checks)"
            },
            {
                "letter": "C",
                "text": "AWS Compute Optimizer"
            },
            {
                "letter": "D",
                "text": "All of the above"
            }
        ],
        "correctAnswerLetter": "D",
        "explanation": "*   **All of the above:** \n    *   **AWS Cost Explorer** provides rightsizing recommendations that help you identify idle and underutilized EC2 instances to reduce costs.\n    *   **AWS Trusted Advisor** inspects your AWS environment and makes recommendations for saving money, improving system availability and performance, and helping to close security gaps. Its cost optimization checks can identify underutilized EC2 instances.\n    *   **AWS Compute Optimizer** recommends optimal AWS Compute resources for your workloads to reduce costs and improve performance by using machine learning to analyze historical utilization metrics.\n    All these services can help identify opportunities for rightsizing and cost optimization of EC2 instances."
    },
    {
        "id": 96,
        "domain": "Design Secure Architectures - Scenario",
        "questionText": "A company wants to ensure that all network traffic to and from their EC2 instances is encrypted in transit, both within their VPC and to the internet. Which combination of services and configurations would best achieve this?",
        "options": [
            {
                "letter": "A",
                "text": "Using HTTPS for all web traffic and SSH for instance access."
            },
            {
                "letter": "B",
                "text": "Implementing AWS VPN connections for all traffic to and from the VPC."
            },
            {
                "letter": "C",
                "text": "Configuring Security Groups to only allow encrypted protocols."
            },
            {
                "letter": "D",
                "text": "Using AWS PrivateLink for traffic within the VPC and a VPN for external traffic."
            }
        ],
        "correctAnswerLetter": "A",
        "explanation": ""
    },
    {
        "id": 97,
        "domain": "Design Resilient Architectures - Scenario",
        "questionText": "A company is using Amazon ElastiCache for Memcached as a caching layer for its web application. They want to improve the fault tolerance of their caching layer. What is a key consideration for Memcached when designing for fault tolerance?",
        "options": [
            {
                "letter": "A",
                "text": "Enable Multi-AZ with automatic failover for the Memcached cluster."
            },
            {
                "letter": "B",
                "text": "Ensure the application logic can gracefully handle cache misses and repopulate the cache from the primary data store if a Memcached node fails."
            },
            {
                "letter": "C",
                "text": "Configure cross-region replication for the Memcached cluster."
            },
            {
                "letter": "D",
                "text": "Increase the number of read replicas for the Memcached cluster."
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **Ensure the application logic can gracefully handle cache misses and repopulate the cache from the primary data store if a Memcached node fails:** ElastiCache for Memcached does not support Multi-AZ automatic failover in the same way Redis does. If a Memcached node fails, the data on that node is lost. Therefore, applications using Memcached should be designed to treat the cache as transient and be able to fetch data from the underlying persistent datastore (e.g., RDS, DynamoDB) and repopulate the cache when a cache miss occurs or a node becomes unavailable. Distributing cache keys across multiple nodes (sharding) also helps limit the impact of a single node failure.\n\n**Why other options are incorrect:**\n\n*   **A) Enable Multi-AZ with automatic failover for the Memcached cluster:** ElastiCache for Memcached does not offer an automatic Multi-AZ failover feature like ElastiCache for Redis.\n*   **C) Configure cross-region replication for the Memcached cluster:** Cross-region replication is not a standard feature for ElastiCache for Memcached for fault tolerance within a region.\n*   **D) Increase the number of read replicas for the Memcached cluster:** Memcached doesn't use a primary/replica model in the same way Redis does for failover. Scaling is typically done by adding more nodes to the cluster and sharding data."
    },
    {
        "id": 98,
        "domain": "Design High-Performing Architectures - Scenario",
        "questionText": "A company has a globally distributed team of developers who need to access a centralized source code repository with low latency. They also require features like pull requests, code reviews, and integration with CI/CD pipelines. Which AWS service is best suited for this requirement?",
        "options": [
            {
                "letter": "A",
                "text": "Amazon S3 with versioning."
            },
            {
                "letter": "B",
                "text": "AWS CodeCommit."
            },
            {
                "letter": "C",
                "text": "Amazon EC2 instances running a self-managed Git server."
            },
            {
                "letter": "D",
                "text": "AWS CodeDeploy."
            }
        ],
        "correctAnswerLetter": "B",
        "explanation": "*   **AWS CodeCommit:** AWS CodeCommit is a fully managed source control service that hosts secure Git-based repositories. It makes it easy for teams to collaborate on code in a secure and highly scalable ecosystem. It eliminates the need to operate your own source control system or worry about scaling its infrastructure. CodeCommit keeps your repositories close to your other resources in AWS, which can help increase the speed and frequency of your development lifecycle. It supports standard Git functionality and works with existing Git tools.\n\n**Why other options are incorrect:**\n\n*   **A) Amazon S3 with versioning:** S3 is object storage and not a Git-based source control system with features like pull requests and code reviews.\n*   **C) Amazon EC2 instances running a self-managed Git server:** This requires manual setup, maintenance, and scaling, which CodeCommit handles as a managed service.\n*   **D) AWS CodeDeploy:** CodeDeploy is a service that automates code deployments to various compute services. It is not a source code repository."
    },
    {
        "id": 99,
        "domain": "Design Cost-Optimized Architectures - Scenario",
        "questionText": "A company is analyzing its AWS bill and notices significant data transfer costs. They want to identify which S3 buckets are contributing most to data transfer out to the internet. Which AWS tool or report provides the most granular information for this analysis?",
        "options": [
            {
                "letter": "A",
                "text": "AWS Trusted Advisor dashboard."
            },
            {
                "letter": "B",
                "text": "Amazon CloudWatch S3 metrics (e.g., BytesDownloaded)."
            },
            {
                "letter": "C",
                "text": "AWS Cost and Usage Report (CUR) filtered by S3 data transfer out, grouped by resource (bucket)."
            },
            {
                "letter": "D",
                "text": "S3 Server Access Logs."
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **AWS Cost and Usage Report (CUR) filtered by S3 data transfer out, grouped by resource (bucket):** The CUR provides the most comprehensive data about your AWS costs and usage. You can load the CUR into a data analysis tool like Amazon Athena or Amazon QuickSight and query it to find detailed information about S3 data transfer costs, including filtering by `productCode: AmazonS3`, `transferType: S3-Out-Bytes`, and grouping by `resourceId` (which would be the S3 bucket name) to identify which buckets are incurring the most data transfer out charges.\n\n**Why other options are incorrect:**\n\n*   **A) AWS Trusted Advisor dashboard:** Trusted Advisor provides recommendations but not the granular cost breakdown needed for this specific analysis.\n*   **B) Amazon CloudWatch S3 metrics (e.g., BytesDownloaded):** CloudWatch metrics can show the volume of data downloaded from a bucket, but the CUR directly links this usage to costs and provides more dimensions for analysis.\n*   **D) S3 Server Access Logs:** These logs provide details about requests made to your S3 bucket, including data transferred. While they contain relevant information, analyzing them for cost attribution across many buckets can be more complex than using the CUR, which is already structured for cost and usage analysis."
    },
    {
        "id": 100,
        "domain": "Design Secure Architectures - Scenario",
        "questionText": "A company needs to provide different levels of access to an Amazon S3 bucket for various IAM users and roles. Some users need read-only access, while others need read-write access to specific prefixes within the bucket. What is the recommended way to manage these permissions according to the principle of least privilege?",
        "options": [
            {
                "letter": "A",
                "text": "Use a single S3 bucket ACL that grants full access to all IAM users, and manage restrictions at the application level."
            },
            {
                "letter": "B",
                "text": "Create separate S3 buckets for each user or role with different access needs."
            },
            {
                "letter": "C",
                "text": "Use a combination of S3 bucket policies and IAM policies to define granular permissions for users and roles based on prefixes."
            },
            {
                "letter": "D",
                "text": "Grant all IAM users administrator access to the S3 service."
            }
        ],
        "correctAnswerLetter": "C",
        "explanation": "*   **Use a combination of S3 bucket policies and IAM policies to define granular permissions for users and roles based on prefixes:** This is the most flexible and secure approach. \n    *   **IAM policies** are attached to users, groups, or roles and define what actions they can perform on which resources (including S3 buckets and objects/prefixes within them).\n    *   **S3 bucket policies** are attached directly to S3 buckets and can grant or deny permissions to principals (AWS accounts, IAM users, roles, services) for actions on the bucket and its objects. \n    By using both, you can implement fine-grained access control that adheres to the principle of least privilege, granting only the necessary permissions for specific tasks and data.\n\n**Why other options are incorrect:**\n\n*   **A) Use a single S3 bucket ACL that grants full access to all IAM users, and manage restrictions at the application level:** S3 ACLs are a legacy mechanism and generally not recommended for complex permission scenarios. Relying solely on application-level restrictions is less secure than enforcing permissions at the AWS resource level.\n*   **B) Create separate S3 buckets for each user or role with different access needs:** This can lead to a proliferation of buckets and become difficult to manage, especially if access needs are complex or overlapping.\n*   **D) Grant all IAM users administrator access to the S3 service:** This violates the principle of least privilege and creates a significant security risk."
    }
]